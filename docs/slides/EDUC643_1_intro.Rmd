---
title: "Intro and regression refresher"
subtitle: "EDUC 643: General Linear Model I"
author: "TBD"
#date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default','new.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

```{R, setup, include = F}
library(pacman)
p_load(here, tidyverse, ggplot2, xaringan, knitr, kableExtra, foreign)

i_am("slides/EDUC643_1_intro.rmd")

# Define color
red_pink <- "#e64173"
turquoise <- "#20B2AA"
orange <- "#FFA500"
red <- "#fb6107"
blue <- "#3b3b9a"
green <- "#8bb174"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
purple <- "#6A5ACD"
slate <- "#314f4f"
# Dark slate grey: #314f4f

# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 6.75,
  fig.width = 10.5,
  warning = F,
  message = F
)
# opts_chunk$set(dev = "svg")
# 
# options(device = function(file, width, height) {
#   svg(tempfile(), width = width, height = height)
# })

options(knitr.table.format = "html")

hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})

```
# Roadmap
<font size = "3">
.pull-left[
1. A refresher on foundations of the General Linear Model
 - Review of bivariate regression
 - Coefficient and model-level inference
 - Correlation and causality
 
2. Assumptions and diagnostics
 - Residuals, studentized, standardized
 - Normality of residuals, homoscedasticity, linearity and independence
 - Diagnostics

3. Multiple regression
 - Concept of statistical adjustment (control)
 - Variance decomposition in MR
 - Measuring effect size in MR
 - Multi-collinearity
]

.pull-right[
4. Categorical predictors
 - Coding dummy variables
 - ANOVA as a special regression model with one categorical predictor
 - Adding a continuous predictor, ANCOVA as a special regression model with a categorical predictor and one or many continuous predictors
 - Variance decomposition when categorical predictors are in the model

5. Interactions and non-linearity
 - Intro to the concept of interaction in MR models
 - Interpreting the interaction between a categorical predictor and continuous predictor
 - Interpreting the interaction between two continuous predictors

6. Model building
]
</font>

---
class: middle, inverse

# Bivariate regression

---

## A motivating question

Using real-world data about the dietary habits, health, and self-appraisals of younger males, we aim to answer the following RQ:
* Does dietary restraint predict BMI among younger adult males?

```{r, echo=T}
do <- read.spss(here::here("data/male_do_eating.sav"), to.data.frame=T) %>% 
  select(Study_ID, EDEQ_restraint, BMI) %>% 
  drop_na()
summary(do$EDEQ_restraint)
summary(do$BMI)
```

---
## Bivariate relationships

We are interested in the *relationship* between dietary restraint and BMI. Statistically, the relationship is the same regardless of which variable is the outcome.

.pull-left[
```{r, echo=F}
ggplot(do, aes(x=BMI, y=EDEQ_restraint)) + 
  geom_point() +
  ggtitle("BMI Predicting Dietary Restraint")
```
]
.pull-right[
```{r, echo = F}
ggplot(do, aes(x = EDEQ_restraint, y = BMI)) + 
  geom_point() +
  ggtitle("Dietary Restraint Predicting BMI")
```
]

---
## Regression Equations

For theoretical reasons, we are choosing to regress BMI (DV) on dietary restraint (IV). 

```{r, echo = F, fig.height=4, fig.width = 6}
lm_plot <- ggplot(do, aes(x=EDEQ_restraint, y=BMI)) + 
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle("Dietary Restraint Predicting BMI")

lm_plot
```

A linear regression equation borrows the mathematical framework for a line to summarize this relationship.
$$\text{BMI} = \beta_0 + \beta_1(\text{Dietary_Restraint})$$
---
## Regression Equations

The regression line describes the mean value of Y for each possible "input" value of X.

$$\text{BMI} = \beta_0 + \beta_1(\text{Dietary_Restraint})$$

--
Our data already provides us with observations of BMI and dietary restraint:

```{r, echo = F}
do %>% 
  select(EDEQ_restraint, BMI) %>% 
  head()
```

--
So, modelling requires us to find the best intercept $(\beta_0)$ and slope $(\beta_1)$ to represent the relationship.

---
## Components of a regression equation

$$BMI = \color{orange}{(\beta_0)} + \color{purple}{(\beta_1)}(\text{Dietary Restraint})$$

<span style = "color:orange"> Intercept $(\beta_0)$: </span> Predicted outcome when X is equal to 0.

--

<span style = "color:purple"> Slope $(\beta_1)$: </span> Predicted increase in the outcome for every one unit increase in X.

--

**Write out the appropriate regression equation for a line that has an intercept of 20 and Dietary Restraint slope of 1.5.**

---
## Components of a regression equation

<span style = "color:orange"> Intercept $(\beta_0)$: </span> Predicted outcome when X is equal to 0.

<span style = "color:purple"> Slope $(\beta_1)$: </span> Predicted increase in the outcome for every one unit increase in X.


$$BMI = \color{orange}{20} + \color{purple}{1.5}(\text{Dietary Restraint})$$
**What is the expected BMI value for a Dietary Restraint rating of 2?**
---
## Components of a regression equation

<span style = "color:orange"> Intercept $(\beta_0)$: </span> Predicted outcome when X is equal to 0.

<span style = "color:purple"> Slope $(\beta_1)$: </span> Predicted increase in the outcome for every one unit increase in X.



$$BMI = \color{orange}{20} + \color{purple}{1.5}(2) = 20 + 3 = 23$$
**For a Dietary Restraint rating of 2, the predicted BMI value is 23.**
---
## Error/Residual Term
"No model is perfect, but some are useful." - George Box

```{r, echo = F, fig.height = 5, fig.width = 7}
lm_plot
```

Is Dietary Restraint a perfect predictor of BMI?

---
## Error/Residual Term
"No model is perfect, but some are useful." - George Box

```{r, echo = F, fig.height = 5, fig.width = 7}
lm_plot
```

Is Dietary Restraint a perfect predictor of BMI? - NO! 

---
## Unmeasured Variables
A "perfect" regression equation would probably include a lot more variables:

$$BMI = \beta_0 + \beta_1(\text{Dietary Restraint}) + \beta_2(\text{Meal Frequency}) + \beta_3(\text{Nutritional Habits})... \beta_\infty$$
Many of these might not even be measurable!

---
## Residual/Error Term

In a regression model, all the variability that we couldn't explain with our predictor is condensed into a <span style="color:green"> residual term $(e_1)$ </span>.

```{r, echo = F, fig.height = 5, fig.width = 7}
fit <- lm(BMI ~ EDEQ_restraint, data=do)
do$predict <- predict(fit)
do %>% 
  sample_n(100) %>% 
  ggplot(aes(x=EDEQ_restraint, y=BMI)) + 
  geom_point() + 
  geom_point(aes(y=predict), col = "blue", alpha=0.3) +
  geom_segment(aes(xend = EDEQ_restraint, yend=predict), col = "green", alpha = 0.5) +
  geom_smooth(aes(EDEQ_restraint, predict))

```

$$BMI = \beta_0 + \beta_1(\text{Dietary Restraint}) + \color{green}{e_1}$$
---
## Sum of Squares

For any observation, the residual is the difference between the observed and predicted value.

$$e_i = Y_i - \hat{Y_i}$$

--

Squaring the residual terms $(e_i^2)$ allows us to treat negative and positive deviations equally, and puts a greater penalty on larger deviations.

```{r, echo = F, comment = NA}
resid_table <- do %>% 
  select(BMI, predict) %>% 
  rename(predicted_BMI = predict) %>% 
  mutate(residual = BMI - predicted_BMI,
         residual_sq = residual^2) %>% 
  mutate(across(everything(), round, 2))

head(resid_table)
```
--

The **sum of squares** is the sum of all our squared residuals:

$$\Large \Sigma(e_i)^2$$

---
## Ordinary Least Squares (OLS) Regression

An OLS-fitted regression finds the best intercept and slope values to minimize the sum of squared residuals.

.pull-left[
```{r, echo = F}
good_fit <- do %>% 
  mutate(predict = (24 + 1.5*EDEQ_restraint))
set.seed(100)
good_fit_plot <- good_fit %>% 
  sample_n(100) %>% 
  ggplot(aes(x=EDEQ_restraint, y=BMI)) + 
  geom_point() + 
  geom_point(aes(y=predict), col = "blue", alpha=0.3) +
  geom_segment(aes(xend = EDEQ_restraint, yend=predict), col = "green", alpha = 0.5) +
  geom_smooth(aes(EDEQ_restraint, predict))

good_fit_plot

```
]

.pull-right[
```{r, echo = F}
bad_fit <- do %>% 
  mutate(predict = (31 + 1.3*EDEQ_restraint))
set.seed(100)
bad_fit_plot <- bad_fit %>% 
  sample_n(100) %>% 
  ggplot(aes(x=EDEQ_restraint, y=BMI)) + 
  geom_point() + 
  geom_point(aes(y=predict), col = "blue", alpha=0.3) +
  geom_segment(aes(xend = EDEQ_restraint, yend=predict), col = "green", alpha = 0.5) +
  geom_smooth(aes(EDEQ_restraint, predict))

bad_fit_plot

```

]

Which regression line appears to be a better fit?

---
## Ordinary Least Squares (OLS) Regression

An OLS-fitted regression finds the best intercept and slope values to minimize the sum of squared residuals.

.pull-left[
```{r, echo = F}
good_fit_plot
```
$\bf{\Sigma(e_i)^2 = 67314.93}$

]

.pull-right[
```{r, echo = F}
bad_fit_plot
```
$\Sigma(e_i)^2 = 100859.80$
]

Using the OLS method, the regression line on the left is a better fit (67,314 < 100,859).

---
## Sum of Squared Residuals (SSR)

SSR is a grander concept than just model predictions. It is a way of thinking about variability.

--

Our outcome's variability is simply the sum of squared deviations from its mean.

$$SSR_\text{BMI} = \sum{(Y_i - \bar{Y})^2}$$
```{r, fig.height = 4, fig.width = 6, echo = F}
do <- do %>% 
  mutate(participant_id = seq(1:1094))

do %>% 
  filter(participant_id <= 30) %>% 
  ggplot(aes(x = participant_id, y = BMI)) +
  geom_point() +
  geom_hline(yintercept = mean(do$BMI), color = "blue") +
  geom_segment(aes(xend = participant_id, yend=mean(do$BMI)), col = "green", alpha = 0.5) +
  geom_label(x = 15, y = mean(do$BMI), label = "Mean BMI", color= "blue")

```
---
## Partitioning Variance

The goal of regression is to account for some of this variability with our model's predictors.

We can partition the outcome's variance into:
* Model-accounted variance
* Residual variance

$$SS_\text{BMI} = SS_\text{Model} + SS_\text{Residual}$$
--

Remember our goal with OLS regression is to find the model coefficients that minimize $SS_\text{Residual}$.


---
# Partitioning Variance
$$\text{If } SS_\text{BMI} = \color{purple}{SS_\text{Model}} + {SS_\text{Residual}}$$

$\color{purple}{SS_\text{Model}}$ is the sum of squared deviations between the model predicted values $(\hat{Y})$ and the mean $(\bar{Y})$.

```{r, echo = F, fig.height = 4, fig.width=6}
do %>% 
  filter(participant_id <= 30) %>% 
  ggplot(aes(x = participant_id, y = BMI)) +
  geom_point() + 
  geom_point(aes(y=predict), col = "purple", alpha=0.3) +
  geom_segment(aes(xend = participant_id, y = mean(do$BMI), yend=predict), col = "purple", alpha = 0.5) +
  geom_hline(yintercept = mean(do$BMI), color = "blue")

```
$$\color{purple}{SS_\text{Model} = \sum{(\hat{Y} - \bar{Y})^2}}$$
---
# Partitioning Variance
$$\text{If } SS_\text{BMI} = \color{purple}{SS_\text{Model}} + \color{green}{SS_\text{Residual}}$$

$\color{green}{SS_\text{Residual}}$ is the sum of squared deviations between the model predicted values $(\hat{Y})$ and the observed values $(Y)$.

```{r, echo = F, fig.height = 4, fig.width=6}
do %>% 
  filter(participant_id <= 30) %>% 
  ggplot(aes(x = participant_id, y = BMI)) +
  geom_point() + 
  geom_point(aes(y=predict), col = "purple", alpha=0.3) +
  geom_segment(aes(xend = participant_id, y = mean(do$BMI), yend=predict), col = "purple", alpha = 0.5) +
  geom_hline(yintercept = mean(do$BMI), color = "blue") +
  geom_segment(aes(xend = participant_id, y = predict, yend=BMI), col = "green", alpha = 0.5)

```
$$\color{green}{SS_\text{Model} = \sum{(Y - \hat{Y})^2}}$$

---
## R-Squared

Partitioning variance can be useful for model evaluation.

$$SS_\text{BMI} = SS_\text{Model} + SS_\text{Residual}$$
$R^2$ is the proportion of variance in the outcome our model accounts for.

$$R^2 = \frac{SS_\text{Model}}{SS_\text{BMI}}$$
--
For example, if $R^2 = 0.30$, then our model accounts for 30% of our sample's variance in BMI.

--
More on this soon.

---
## Evaluating a regression

Let's try fitting our regression in R.

```{r}
fit <- lm(BMI ~ EDEQ_restraint, do)
summary(fit)
```


---
## Evaluating a regression - Coefficients

Here we can find our intercept and slope coefficients for our linear regression.

```{r, highlight.output = c(4,5 ), echo = F, output.lines = -(1:8), comment = NA}
summary(fit)

```

**The predicted BMI for a young male with a dietary restraint rating of 0 is 23.94 $(\beta_0)$.**

--

**For each one unit increase in dietary restraint, there is an expected increase of 1.04 $(\beta_1)$ in BMI.**

---
## Evaluating a regression - Residuals

Here is our summary of the residual error.

```{r, highlight.output = 9, echo = F, output.lines = -(1:8), comment = NA}
summary(fit)

```

The residual standard error (RSE) is the standard deviation of the residuals. This summarizes the variability of observed values around the model-predicted values, **in the original units of the DV.**

$$RSE = 6.081$$

This means observed values vary around our model-predicted BMI with a standard deviation of 6.081. In BMI, 6 units is quite large!

---
## Evaluating a regression - R-Squared

Here is our summary of model performance.

```{r, highlight.output = 10, echo = F, output.lines = -(1:8), comment = NA}
summary(fit)

```

The R-squared value is .05.

This means that our model accounts for 5% of the variance in BMI. Since our model has only one predictor, we can alternatively say Dietary Restraint accounts for 5% of the variance in BMI.

---
class: middle, inverse

# Regression inference

---
## What are we testing in a regression?

There are multiple inferential tests in a regression model:
* Test of the model (omnibus test) 
* Tests of the coefficients

```{r, highlight.output = c(4, 5, 11), echo = F, output.lines = -(1:8), comment = NA}
summary(fit)

```


---
### F-Distributions and Omnibus Tests

The omnibus test uses the F-distribution to test the ratio of two variances (i.e., explained vs unexplained variance).

Null hypothesis: The model does not account for variance in Y.

If we reject the null, then the model accounts for more variance than we would expect by chance. 
---
### F-Distributions and Omnibus Tests

Just like tests with other probability distribution, we are testing the probability of obtaining a value (or more extreme value) of F under the F-Distribution.

$$\Large F = \frac{MS_{Model}}{MS_{residual}}$$

Mean Squares (MS) are the Sums of Squares divided by their respective degrees of freedom.

---
# Interpreting the Omnibus Test

```{r, highlight.output = c(11), echo = F, output.lines = -(1:8), comment = NA}
summary(fit)

```

Our omnibus test was significant (p < .001), therefore we can reject the null hypothesis that the model accounts for zero variance in BMI.

---
### Regression Coefficients

Significance tests of regression coefficients test the null hypothesis that $\beta = 0$. (there is no linear relationship between the predictor and the outcome).

$$\Large H_{0}: \beta_{1}= 0$$
$$\Large H_{1}: \beta_{1} \neq 0$$
Here, the p-value refers to the probability of obtaining a slope equal to or more extreme than $b$ assuming $H_0$ is true. 

---
### Regression Coefficients

Our R output shows us the significance of the intercept. Typically, we are not interested in whether the intercept significantly differs from 0.

```{r, highlight.output = c(4), echo = F, output.lines = -(1:8), comment = NA}
summary(fit)
```

---
### Regression Coefficients

```{r, highlight.output = c(5), echo = F, output.lines = -(1:8), comment = NA}
summary(fit)
```

$$t = \frac{\hat{\beta_1} - 0}{SE(\hat{\beta_1})} = \frac{1.04}{.14} = 7.43$$
$$Pr(t < -7.43 \text{ or } t > 7.43)|H_0 = 0.000000000000048 \text{ or p <.001}$$
---
### Regression Coefficients

```{r, highlight.output = c(5), echo = F, output.lines = -(1:8), comment = NA}
summary(fit)
```

Under the null hypothesis, it is extremely unlikely to obtain a Dietary Restraint slope of 1.04 (p < .001). Therefore, we can reject the null and conclude that there is a significant relationship between Dietary Restraint rating and BMI.
---
### Inference in Regression

In a simple regression model, we are testing for a linear relationship between a predictor and the outcome.

```{r, echo = F, fig.height = 4, fig.width = 6}
set.seed(555)
quad <- tibble(x = rnorm(1000, 0, 1),
                  y = x^2 + runif(1000, -0.1, 0.1))

ggplot(quad, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  theme_minimal()
```

It is important to inspect the data to make sure we are modelling correctly. Otherwise, we'd miss this non-linear relationship!
---
## Summarizing Regression Results

```{r, highlight.output = c(5), echo = F, output.lines = -(1:8), comment = NA}
summary(fit)
```

A simple regression model was used to describe the relationship between BMI and Dietary Restraint in young adult males. Dietary Restraint was found to be a significant predictor (p < .001) and accounted for approximately 5.07% of the variance in BMI. For each one unit increase in Dietary Restraint, there is an average increase of 1.04 BMI units.

---
### Regressions as a prediction

Regression equations can be used to evaluate the relationship between variables, and to predict expected values based on particular values of our IVs.

We can ask: What is the expected BMI value for a young male with a Dietary Restraint rating of **4**?

--
$$\Large BMI = 23.94 + 1.04*(\boldsymbol{4}) = 28.1$$
The expected BMI for a Dietary Restraint of 4 is 28.1

--

Technically, there is no limit to what we can input!

---
### Predicting beyond your data

Regression equations can be used to evaluate the relationship between variables, and to predict expected values based on particular values of our IVs.

We can ask: What is the expected BMI value for a young male with a Dietary Restraint rating of **400**?

--

Using our measure, this is not a possible value of Dietary Restraint but we can still estimate BMI using our regression equation.

--
$$\Large BMI = 23.94 + 1.04*(\boldsymbol{400}) = 439.94$$
An adult would have to weight 2,894 pounds at 5' 8" to get a BMI of 439.94.

--

**Only predict within the bounds of your data.**

---
class: middle, inverse

# Correlation and causality

---
## Oregon Divorces and Beef Consumption

Does U.S. beef consumption cause more "beefs" between Oregonians and their spouses?
* What is the relationship between Oregon's per capita divorce rate and U.S. per capita beef consumption? 

--
```{r, echo = F, fig.height = 4, fig.width= 6}
beef <- read_csv(here::here("data/divorce_beef.csv"))

ggplot(beef, aes(divorce_rate, beef_consumption)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  theme_minimal() +
  ggtitle("Oregon Divorce Rate and U.S. Beef Consumption (2000-2009)")
```

There certainly seems to be a relationship! Let's run a simple regression model.
---
## Regression Models -  Divorce and Beef

If we regress divorce rate on beef consumption...

```{r, highlight.output = c(5), echo = F, output.lines = -(1:8), comment = NA}
m_beef <- lm(divorce_rate ~ beef_consumption, beef)
summary(m_beef)
```

Beef consumption accounts for 50% of the variance in Oregon's divorce rate!

---
## Regression Models -  Divorce and Beef

If we regress U.S. beef consumption on Oregon's divorce rate...

```{r, highlight.output = c(5), echo = F, output.lines = -(1:8), comment = NA}
m_beef <- lm(beef_consumption ~ divorce_rate, beef)
summary(m_beef)
```

Oregon's divorce rate accounts for 50% of the variance in U.S. beef consumption?
---
## Regression Models -  Divorce and Beef

What can we conclude? - Do increases in Oregon's divorce rate **cause** increases in U.S. beef consumption, or do increases in beef consumption **cause** increases in the divorce rate?

--

**Probably neither...**

--

We tested for a linear relationship. There is a significant linear relationship or correlation between divorce rate and beef consumption, but we did not conduct an experimental study.

**We cannot establish causality, nor the direction of the relationship with a statistical test.**

--

Study design is critical.

---
## Turn and Talk

* Discuss science communication and how you would distinguish correlations from causal relationships to the average person.

* In what contexts might it be difficult to conduct an experimental study to establish causality?

---

End of Slides
