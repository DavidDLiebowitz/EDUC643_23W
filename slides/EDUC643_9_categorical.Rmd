---
title: "Categorical predictors and ANOVA/ANCOVA"
subtitle: "EDUC 643: Unit 4"
author: "David D. Liebowitz"
#date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'uo', 'ki-fonts', 'my_custom.css', 'xaringanthemer.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{R, setup, include = F}
library(pacman)
p_load(here, tidyverse, xaringan, knitr, kableExtra, haven, broom, xaringanthemer, reshape2, car)

i_am("slides/EDUC643_9_categorical.rmd")



red_pink <- "#e64173"
turquoise = "#20B2AA"
orange = "#FFA500"
red = "#fb6107"
blue = "#3b3b9a"
green = "#8bb174"
grey_light = "#B3B3B3"
grey_mid = "#7F7F7F"
grey_dark = "grey20"
purple = "#6A5ACD"
slate = "#314f4f"

extra_css <- list(
  ".red"   = list(color = "red"),
  ".blue"  =list(color = "blue"),
  ".red-pink" = list(color= "#e64173"),
  ".gray" = list(color= "#B3B3B3"),
  ".purple" = list(color = "purple"),
  ".orange" = list(color = "#FFA500"),
  ".small" = list("font-size" = "90%"),
  ".large" = list("font-size" = "120%"),
  ".tiny" = list("font-size" = "70%"),
  ".tiny2" = list("font-size" = "50%"))


write_extra_css(css = extra_css, outfile = "my_custom.css")

options(htmltools.dir.version = FALSE)

hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})

knitr::opts_chunk$set(warning = FALSE,
                      message = FALSE,
                      echo = FALSE,
                      fig.align = "center",
                      fig.height = 6.5,
                      fig.width = 10)


```

# Roadmap
```{r, echo=F, out.width="90%"}
include_graphics("Roadmap4.jpg")

dibels <- read.csv(here("data/dibels.csv"))
dibels_long <- read.csv(here("data/dibels_long.csv"))

```
---
# Unit goals

- Describe the relationship between dichotomous and polychotomous variables and convert variables between these forms, as necessary
- Conduct a two-sample $t$-test
- Describe the relationship between a two-sample $t$-test and regressing a continuous outcome on a dichotomous predictor
- Estimate a regression with one dummy variable as a predictor and interpret the results (including when the reference category changes)
- Estimate a multiple regression model with several continuous and dummy variables and interpret the results
- Estimate an ANOVA model and interpret the within- and between-group variance
 + Do the same for an ANCOVA model, adjusting for additional continuous predictors
- Describe the similarities and differences of Ordinary-Least Squares regression analysis and ANOVA/ANCOVA, and when one would prefer one approach to another
- Describe potential Type I error problems that arise from multiple group comparisons and potential solutions to these problems, including theory, pre-registration, ANOVA and *post-hoc* corrections
- Describe the relationship between different modeling approaches with the General Linear Model family

---
class: middle, inverse

# Categorical variables

---
# Categorical variables
So far, we have only looked at General Linear Models (and their associated OLS regression estimating equations) involving continuous predictors. But what about .red-pink[**categorical predictors**]?

--

What are categorical predictors?
- Categorical predictors are ***predictors whose values denote categories***.

--
Of course, this begs the question...

---
# Categorical predictors

### Important distinctions and conventions:

.small[
.pull-left[
**Nominal predictors**
- These have *unordered* values
- E.g., gender, religion, political party
]

.pull-left[
**Ordinal predictors**
- These have *ordered* values
- E.g., grade, developmental stage, education level (?)
]
]

--

Another important distinction: .red-pink[**dichotomies**] (only 2 categories) vs. .red-pink[**polychotomies**] (>2 categories)

---
# Our (new!) motivating question
.small[A team of researchers based at the .green[**University of Oregon**] aimed to understand the effects of the COVID-19 pandemic on students' early literacy skills.<sup>1</sup>]
```{r}
include_graphics("dibels_team.png")
```

.small[Ann Swindells Professor in Special Education [Gina Biancarosa](https://education.uoregon.edu/directory/faculty/all/ginab), former UO doctoral students David Fainstein, Chris Ives, and Dave Furjanic, along with CTL Research Manager Patrick Kennedy, used four waves of Dynamic Indicators of Basic Early Literacy Skills (DIBELS) data to analyze the extent to which students' Oral Reading Fluency (ORF) scores differed across four waves of DIBELS assessment prior-to and during the pandemic.]

.small[Their study is now forthcoming at the *Elementary School Journal*.]

.footnote[[1] For various reasons, the pandemic is a ["lousy natural experiment"](https://www.educationnext.org/covid-19-pandemic-lousy-natural-experiment-for-studying-the-effects-online-learning/) for examining the effects of a particular policy response (e.g, virtual schooling). However, it is quite possible to seek to understand its global effects via just the type of analysis Furjanic et al. conducted.]

---
# Our data
```{r, echo=T}
str(dibels)
```

---
# Mean comparison
```{r, echo=T}
mean(dibels$pre)
mean(dibels$post)
```

---
## Understanding the distributions

.pull-left[
**Pre-pandemic**
```{r}
boxplot(dibels$pre,
        ylim =c(0,170))
```
]

.pull-right[
**Post-pandemic onset**
```{r}
boxplot(dibels$post,
        ylim =c(0,170))
```
]

---
# Testing for differences

```{r, echo=T}
t.test(dibels$pre, dibels$post,
       var.equal=T, paired=T)
```

---
# Data structure
```{r}
library(DT)
datatable(dibels[,c(1:4, 11:12)], fillContainer = FALSE, 
              options = list(pageLength = 9)) %>%
              formatRound("y1_boy_mean", digits=1) %>%
              formatRound("y1_moy_mean", digits=1) %>%
              formatRound("pre", digits=1) %>%
              formatRound("post", digits=1)
```

---
## Categorical Variables - Levels

A categorical variable is comprised of *K* levels.

```{r, echo=T}
#levels(do$grade)
```

Here we have seven different levels of marital status.

--

We need to represent them using numerical values, but these levels don't inherently have a numerical structure.

--

So... we use **dummy coding**.

---
## Dummy Coding

The most common process for representing categorical variables in regression is dummy coding. 

* In short, dummy coding essentially creates a new (dummy-coded) variable for each level.

|Marital Status   |D1   |D2   |D3   | ...
|------------------------------------------------
|Single           |0    |0    |0    |...
|Engaged          |1    |0    |0    |...
|Married          |0    |1    |0    |...
|...              |0    |0    |1    |...


* One group becomes the reference group (in this case "Single").

* The dummy-coded variables are then coded "1" for their corresponding level, and 0 for all other levels.

---
## Dummy Coding

In a sample dataset, we could visualize the dummy coding scheme like this:

|Name    |Marital Status   |D1 (Engaged) |D2 (Married) |...
|------------------------------------------------
|Jacob   |Single           |0            |0             |...
|Mitsuo  |Married          |0            |1             |...
|Eduardo |Engaged          |1            |0             |...
|Yasir   |Single           |0            |0             |...


Since "Single" is our reference, we don't create a column (its implied by 0's in all other groups).

Hence, we have *K*-1 dummy-coded variables. 


---
class: middle, inverse

# Categorical predictors in regression

---
# Categorical predictors in regression

In a regression model, categorical predictors are typically entered in their dummy-coded format.

$$\hat{Y} = b_0 + b_1D_2 + b_2D_3 + b_3D_2...$$
--

In our marital status regression, we can think of the equation like this:

$$\hat{BMI} = b_0 + b_1\text{(Engaged)} + b_2\text{(Married)} + b_3\text{(Widowed)}...b_6\text{(Divorced)}$$
With seven levels of marital status, our equation has only six betas because one group ("Single") is our reference.

---
# Categorical Coefficients

$$\Large \hat{BMI} = b_0 + b_1\text{(Engaged)} + b_2\text{(Married)} + ...b_6\text{(Divorced)}$$

* **If "Single" is our reference, what might the intercept $(b_0)$ mean?**

--

> The intercept $(b_0)$ is the value of $\hat{Y}$ when all predictors = 0. With only dummy codes in our equation, the intercept is the mean of the reference group ("Single").

--

* **How might you interpet the slope coefficients, such as $b_1$?**

--

> $b_1$ represents the average effect, or change in BMI, from being in the Engaged group... **relative to the reference group.**

---
## Interpreting Coefficients

```{r, output.lines = -(1:8), highlight.output = 4, comment = NA}
fit <- lm(mean_orf ~ period, dibels_long)
summary(fit)
```

**On average, individuals with a single martial status had an average BMI of 25.31.**

---
## Interpreting Coefficients
```{r, echo = F, output.lines = -(1:8), highlight.output = 5, comment = NA}
summary(fit)
```

On average, individuals with a single martial status had an average BMI of 25.31.

**Participants with divorced marital status had an average BMI that was 2.97 higher than those with a single marital status.**

---
## Interpreting Coefficient Significance

Coefficient significance tests still test the null hypothesis $\beta = 0$, but **we are testing against the reference group** implicit in our intercept.

```{r, echo = F, output.lines = -c(1:8, 18:24), comment = NA}
summary(fit)
```

**Which marital status group significantly differs from our reference group - "single"?**

---
## Interpreting Coefficient Significance

Coefficient significance tests still test the null hypothesis $\beta = 0$, but **we are testing against the reference group** implicit in our intercept.

So, this just a comparison of means, or an independent-samples t-test!

--

```{r, echo = F, output.lines = -c(1:8, 18:24), highlight.output = 10, comment = NA}
summary(fit)
```

**Which marital status group significantly differs from our reference group - "single"?**
 
> Widowed participants significantly differed from individuals with a single marital status by an average BMI of 16.61 (p <.001).

---
## Prediction with Categorical Variables

Using the coefficients from our R output, we have the following regression equation (abbreviated for space):

$$\hat{BMI} = 25.31 + -2.32\text{(Engaged)} + 0.30\text{(Married)} + ...+ 16.61\text{(Widowed)}$$
**What is the predicted BMI for participants in the married category?**

--
$$\hat{BMI} = 25.31 + -2.32\text{(0)} + 0.30\text{(1)} + ...+ 16.61\text{(0)} = 25.31 + 0.30 = 25.61$$
For dummy coded variables, we just add the appropriate effects for the group we are interested in, or omit them if they are in our reference group.

---
class: middle, inverse

# ANOVA

---
## ANOVA

* Analysis of variance, or ANOVA, is a special case of the general linear model.

* The primary goal of ANOVA is a comparison of means across different groups.
  + $H_0: \mu_1 = \mu_2 = \mu_3... \mu_K$
  
* Although regression frameworks are more the norm across most disciplines, the ANOVA approach can be useful for:
  + Testing the main effects of categorical variables
  + Exploring their associated variance in the outcome

---
## ANOVA vs. Regression

* A regression with dummy indicator variables is mathematically identical to ANOVA.

* The F-test in a regression model represents a test of the model's variance against the residual.

* In ANOVA, we can have one or more F-tests where we "batch test" a group of coefficients.
  + Example: Assuming we had multiple variables in our model, we could test the main effect of marital status against the residual rather than examine each individual coefficient.

---
## Disordered Eating Data

Since some of our marital status groups are poorly represented in our dataset (n < 20), we will only examine the differences between the following groups: a) single, b) engaged, c) married, and d) single in a committed relationship.




---
## Partitioning Variance

In regression, we partition our total variance $SS_\text{total}$ into our $SS_\text{model}$ and $SS_\text{error}$.

$SS_\text{model}$ = Deviation of observed value from the predicted value $(Y_{i}-\hat{Y}_i)$.

 $SS_\text{error}$ = Deviation of predicted value from the grand mean $(\hat{Y} - \bar{Y}_i)$.

--

In ANOVA, we apply a similar but slightly different conceptual process.
---
## Partioning Variance in ANOVA

In ANOVA, we separate variance into between-group and within-group variance.

$SS_\text{within}$ = Deviation of observed value from its group mean $(Y_{ik}-\bar{Y}_k)$.

$SS_\text{between}$ = Deviation of group mean from the grand mean $(\bar{Y}_k - \bar{Y})$.

--

$$SS_\text{total} = SS_\text{within} + SS_\text{between}$$
---
## Visualizing ANOVA

```{r, echo = F, fig.height=4, fig.width=6, echo = F}
# 
# m <- lm(BMI ~ marital_status, do2)
# 
# do2 <- do2 %>% 
#   group_by(marital_status) %>% 
#   mutate(group_mean = mean(BMI)) %>% 
#   ungroup()
# 
# do_samp <- do2 %>% 
#   group_by(marital_status) %>% 
#   sample_n(20)
# 
# do_samp$predict <- predict(m, do_samp)
# 
# do_samp$jitter_marit <-
#   ave(as.numeric(do_samp$marital_status), 
#       do_samp$marital_status, 
#       FUN = function(x) x + rnorm(length(x), sd = .2))
# 
# do_samp <- do_samp %>% 
#   mutate(jitmin = min(jitter_marit),
#          jitmax = max(jitter_marit))
# 
# ```
# 
# .pull-left[
# ```{r, fig.height=3, fig.width=5, echo = F}
# with_plot <- do_samp %>% 
# ggplot(aes(x = jitter_marit, xend = jitter_marit,
#                  y = BMI, yend = predict)) +
#     geom_segment(color = "green") +
#     geom_point() +
#     scale_x_continuous("Marital Status", 
#                        breaks = c(1,2,3,4),
#                        labels = levels(do2$marital_status)) +
#   geom_segment(aes(x = jitmin, xend = jitmax, y = predict, yend = predict),
#                color = "darkgreen") +
#   ylim(0, 40) +
#   theme_minimal() +
#   ggtitle("Within-Groups Variance")
# 
# with_plot
```

This shows the residual variance around the group means. Just like the error term, it is all the remaining variance our predictor can't explain.
]

.pull-right[
```{r, fig.height=3, fig.width=5, echo = F}
# btw_plot <- ggplot(do_samp, aes(x = marital_status, y = group_mean)) +
#   geom_point(aes(x = as.numeric(marital_status) + 0.5,
#                    y = group_mean), color = "darkgreen") +
#     geom_segment(aes(x = as.numeric(marital_status),
#                    xend = as.numeric(marital_status),
#                    y = group_mean,
#                    yend = mean(do2$BMI)),
#                  linetype = 2) +
#   geom_hline(yintercept = mean(do2$BMI), color = "black", linetype = 2) +
#       scale_x_continuous("Marital Status", 
#                        breaks = c(1, 2, 3, 4),
#                        labels = levels(do2$marital_status)) +
#   ylim(0, 40) +
#   theme_minimal() +
#   ggtitle("Between-Groups Variance")
# 
# btw_plot
```

This shows the group effect of marital status against the grand mean.
]


---
## ANOVA Test Statistic

When we conduct an ANOVA we are testing the significance of an F statistic using the following formula:

$$\large F = \frac{MS_\text{between}}{MS_\text{within}}$$

The mean squares (MS) of between- and within-group variance is just their Sums of Squares divided by their degrees of freedom.

.pull-left[
$$\large MS_w = \frac{SS_w}{df_w}$$
$$\large df_w = N-G$$

]
.pull-right[
$$\large MS_b = \frac{SS_b}{df_b}$$
$$\large df_b = G-1$$
]
---
## ANOVA Significance Test

The null hypothesis of an ANOVA regards the ratio of between- to within-group variance.

Essentially, we are asking if the mean square variance of the group means around the grand mean is significantly greater than the mean square variance of observations around their group mean.
--
If the between-group variance were much larger than the within-group variance, than the F-statistic would exceed 1.

$$\large F = \frac{MS_\text{between}}{MS_\text{within}} = \frac{4.3}{1.5} = 2.87$$

If the between-group variance is equal to or much smaller than the within-group variance, than our F statistic will be $\le$ 1.

$$\large F = \frac{MS_\text{between}}{MS_\text{within}} = \frac{0.2}{1.5} = 0.13$$

---
## Calculating the F-Statistic

Let's find our F-statistic for our marital status variable. 

```{r, echo = F}
#do2$predicted_BMI <- predict(m, do2)
```

#### Within-Group (Residual) Variance $MS_\text{Within}$
```{r}
# total n - number of groups (4)
#df_within <- 1079 - 4

#sum((do2$BMI - do2$group_mean)^2) / df_within
```

#### Between-Group Variance $MS_\text{Between}$
```{r}
# number of groups (4) - 1
#df_btw <- 4-1

#sum((mean(do2$BMI) - do2$group_mean)^2) / df_btw
```

---
## Calculating the F-Statistic

$$MS_\text{Between} = 41.76$$
$$MS_\text{Within} = 37.12$$

$$F = \frac{MS_\text{Between}}{MS_\text{Within}} = \frac{41.76}{37.12} = 1.125$$ 
Our F-statistic is 1.125. Now that we see how it is calculated, let's run an ANOVA in R to get our p-value and review the output.

---
## ANOVA Output

Because ANOVA is just a particular method of analyzing variance in GLMs, we can wrap `anova` around our `lm` model.

```{r, highlight.output = 6}
# m1 <- lm(BMI ~ marital_status, do2)
# car::Anova(m1, type = 3)
```
Here we can see all the information for the residual and marital status we calculated earlier. Only the p-value is new!

--

With a p-value of .34, our F-statistic is not that unusual. Therefore we fail to reject the null hypothesis. The marital status group means do not significantly differ in terms of BMI.


---
class: middle, inverse

# ANCOVA

---
# ANCOVA

* Analysis of covariance (ANCOVA) is an extension of ANOVA.
* Essentially, it is the an ANOVA that controls for the effects of other **continuous** independent variables.
* The null hypothesis is still the same as ANOVA $(\mu_1 = \mu_2 = \mu_K)$.

Sample Research Question: What is the difference in BMI among different marital status groups, **after controlling for age?**

---
## ANCOVA as Regression

Research Question: What is the difference in BMI among different marital status groups, **after controlling for age?**


In a regression equation, categorical variables would still be represented as a dummy-coded variable. We are just adding an additional $\beta$ for our covariate.

--

$$BMI_i = \beta_0 + \beta_1\text{Marital Status}_i + \beta_2\text{Age}_i + \epsilon_i$$

--

$$BMI_i = \beta_0 + \beta_1\text{Single_Commited}_i + \beta_2\text{Engaged}_i + \beta_3\text{Married}_i + \beta_4\text{Age}_i + \epsilon_i$$
--

Now we have to remember to qualify our interpretations given our covariates, or control variables.

$\beta_3$: The predicted difference in BMI between a single male and a married male, after controlling for age.

---
## ANCOVA



---
## ANCOVA Output

Now that we have an additional covariate in the model, we can expect a different residual sums of squares.

```{r, highlight.output = 8,}


```

* In our basic ANOVA, our residual SS was 39,907. Now we see the residual is 39,224. 

* Our variance has been "reorganized" with the addition of `age_year`. 

---
## ANCOVA Output

```{r, highlight.output = c(6, 8)}

```

* Our F-test for marital status still regards the relationship between its SS and the residual SS, but we have now accounted for the variance associated with age.

* Still, our resulting F-value is not significant (p = 0.28), meaning marital status does not significantly account for BMI after controlling for age.

* On the other hand, the main effect of age is significant (p < .001)

---
## Regression Output

We can compare our ANCOVA output to our regression output and see our dummy-coded, "unbatched", analysis of marital status effects. 

```{r, echo = F, highlight.output = c(12, 13, 14), comment = NA}

```
---

# Variance decomposition

* ANOVA (and ANCOVA) are typically used as methods for analyzing variance associated with group or categorical effects.

* Repeated Measures ANOVA is a similar process, but we define our "groups" as conditions within an individual.
  + e.g., Baseline, Treatment 1, Treatment 2

* ANOVA can be particularly useful for analyzing a regression model's terms, rather than its individual coefficients.



---
## It was the GLM the whole time...
```{r}
include_graphics("glm_scooby.png")
```


---
## Putting categorical predictors together




---
class: middle, inverse
# Synthesis and wrap-up

---
# Goals for the unit

- Describe the relationship between dichotomous and polychotomous variables and convert variables between these forms, as necessary
- Conduct a two-sample $t$-test
- Describe the relationship between a two-sample $t$-test and regressing a continuous outcome on a dichotomous predictor
- Estimate a regression with one dummy variable as a predictor and interpret the results (including when the reference category changes)
- Estimate a multiple regression model with several continuous and dummy variables and interpret the results
- Estimate an ANOVA model and interpret the within- and between-group variance
 + Do the same for an ANCOVA model, adjusting for additional continuous predictors
- Describe the similarities and differences of Ordinary-Least Squares regression analysis and ANOVA/ANCOVA, and when one would prefer one approach to another
- Describe potential Type I error problems that arise from multiple group comparisons and potential solutions to these problems, including theory, pre-registration, ANOVA and *post-hoc* corrections
- Describe the relationship between different modeling approaches with the General Linear Model family


---
# To-Dos

### Reading: 
- **Finish by Feb. 16**: LSWR Chapter 14 and 16.6

### Assignment 3:
- Due Feb. 14, 11:59pm (**note extension**)

### Assignment 4:
- Due Feb. 28, 11:59pm
