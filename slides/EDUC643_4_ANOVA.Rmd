---
title: "Categorical Predicators and ANOVA"
subtitle: "EDUC 643: General Linear Model I"
author: "TBD"
#date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'uo', 'ki-fonts', 'my_custom.css', 'xaringanthemer.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false

---

```{R, setup, include = F}
library(pacman)
p_load(here, tidyverse, ggplot2, xaringan, knitr, kableExtra, foreign, broom, xaringanthemer, reshape2, car)

i_am("slides/EDUC643_4_ANOVA.rmd")


extra_css <- list(
  ".red"   = list(color = "red"),
  ".blue"  =list(color = "blue"),
  ".green" = list(color = "#8bb174"),
  ".purple" = list(color = "#6A5ACD"),
  ".red-pink" = list(color= "#e64173"),
  ".grey-light" = list(color= "grey70"),
  ".slate" = list(color="#314f4f"),
  ".small" = list("font-size" = "90%"))

write_extra_css(css = extra_css, outfile = "my_custom.css")

# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 6.75,
  fig.width = 10.5,
  warning = F,
  message = F
)
opts_chunk$set(dev = "svg")
options(device = function(file, width, height) {
  svg(tempfile(), width = width, height = height)
})
options(knitr.table.format = "html")


hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})


```



```{r, data import, echo = F}
do <- read.spss(here::here("data/male_do_eating.sav"), to.data.frame=T) %>% 
  select(Study_ID, BMI, marital_status_group, education_group, age_year) %>% 
  rename(marital_status = marital_status_group) %>% 
  mutate(marital_status = as.character(marital_status),
         marital_status = case_when(marital_status == "single but committed relationship" ~ "single_committed",
                                    marital_status == "married but separated" ~ "married_separated",
         TRUE ~ marital_status),
         marital_status = relevel(as.factor(marital_status), ref = "single")) %>% 
  drop_na()

```

---
# Roadmap
```{r, echo=F, out.width="90%"}
include_graphics("Roadmap4.jpg")
```
---
class: middle, inverse

# Categorical variables

---
## Categorical Variables
So far, we have only looked at regression equations involving continuous predictors. But what about **categorical predictors**?

--
Returning to our young men's disordered eating data, we want to know:
* What is the effect of *marital status* on BMI?
* What is the relationship between *educational level* and BMI?

---
## Categorical Variables - Levels

A categorical variable is comprised of *K* levels.

```{r, comment = NA}
levels(do$marital_status)
```

Here we have seven different levels of marital status.

--

We need to represent them using numerical values, but these levels don't inherently have a numerical structure.

--

So... we use **dummy coding**.

---
## Dummy Coding

The most common process for representing categorical variables in regression is dummy coding. 

* In short, dummy coding essentially creates a new (dummy-coded) variable for each level.

|Marital Status   |D1   |D2   |D3   | ...
|------------------------------------------------
|Single           |0    |0    |0    |...
|Engaged          |1    |0    |0    |...
|Married          |0    |1    |0    |...
|...              |0    |0    |1    |...


* One group becomes the reference group (in this case "Single").

* The dummy-coded variables are then coded "1" for their corresponding level, and 0 for all other levels.

---
## Dummy Coding

In a sample dataset, we could visualize the dummy coding scheme like this:

|Name    |Marital Status   |D1 (Engaged) |D2 (Married) |...
|------------------------------------------------
|Jacob   |Single           |0            |0             |...
|Mitsuo  |Married          |0            |1             |...
|Eduardo |Engaged          |1            |0             |...
|Yasir   |Single           |0            |0             |...


Since "Single" is our reference, we don't create a column (its implied by 0's in all other groups).

Hence, we have *K*-1 dummy-coded variables. 


---
class: middle, inverse

# Categorical predictors in regression

---
# Categorical predictors in regression

In a regression model, categorical predictors are typically entered in their dummy-coded format.

$$\hat{Y} = b_0 + b_1D_2 + b_2D_3 + b_3D_2...$$
--

In our marital status regression, we can think of the equation like this:

$$\hat{BMI} = b_0 + b_1\text{(Engaged)} + b_2\text{(Married)} + b_3\text{(Widowed)}...b_6\text{(Divorced)}$$
With seven levels of marital status, our equation has only six betas because one group ("Single") is our reference.

---
# Categorical Coefficients

$$\Large \hat{BMI} = b_0 + b_1\text{(Engaged)} + b_2\text{(Married)} + ...b_6\text{(Divorced)}$$

* **If "Single" is our reference, what might the intercept $(b_0)$ mean?**

--

> The intercept $(b_0)$ is the value of $\hat{Y}$ when all predictors = 0. With only dummy codes in our equation, the intercept is the mean of the reference group ("Single").

--

* **How might you interpet the slope coefficients, such as $b_1$?**

--

> $b_1$ represents the average effect, or change in BMI, from being in the Engaged group... **relative to the reference group.**

---
## Interpreting Coefficients

```{r, echo = F, output.lines = -(1:8), highlight.output = 4, comment = NA}
m_cat <- lm(BMI ~ marital_status, do)
summary(m_cat)
```

**On average, individuals with a single martial status had an average BMI of 25.31.**

---
## Interpreting Coefficients
```{r, echo = F, output.lines = -(1:8), highlight.output = 5, comment = NA}
summary(m_cat)
```

On average, individuals with a single martial status had an average BMI of 25.31.

**Participants with divorced marital status had an average BMI that was 2.97 higher than those with a single marital status.**

---
## Interpreting Coefficient Significance

Coefficient significance tests still test the null hypothesis $\beta = 0$, but **we are testing against the reference group** implicit in our intercept.

```{r, echo = F, output.lines = -c(1:8, 18:24), comment = NA}
summary(m_cat)
```

**Which marital status group significantly differs from our reference group - "single"?**

---
## Interpreting Coefficient Significance

Coefficient significance tests still test the null hypothesis $\beta = 0$, but **we are testing against the reference group** implicit in our intercept.

So, this just a comparison of means, or an independent-samples t-test!

--

```{r, echo = F, output.lines = -c(1:8, 18:24), highlight.output = 10, comment = NA}
summary(m_cat)
```

**Which marital status group significantly differs from our reference group - "single"?**
 
> Widowed participants significantly differed from individuals with a single marital status by an average BMI of 16.61 (p <.001).

---
## Prediction with Categorical Variables

Using the coefficients from our R output, we have the following regression equation (abbreviated for space):

$$\hat{BMI} = 25.31 + -2.32\text{(Engaged)} + 0.30\text{(Married)} + ...+ 16.61\text{(Widowed)}$$
**What is the predicted BMI for participants in the married category?**

--
$$\hat{BMI} = 25.31 + -2.32\text{(0)} + 0.30\text{(1)} + ...+ 16.61\text{(0)} = 25.31 + 0.30 = 25.61$$
For dummy coded variables, we just add the appropriate effects for the group we are interested in, or omit them if they are in our reference group.

---
class: middle, inverse

# ANOVA

---
## ANOVA

* Analysis of variance, or ANOVA, is a special case of the general linear model.

* The primary goal of ANOVA is a comparison of means across different groups.
  + $H_0: \mu_1 = \mu_2 = \mu_3... \mu_K$
  
* Although regression frameworks are more the norm across most disciplines, the ANOVA approach can be useful for:
  + Testing the main effects of categorical variables
  + Exploring their associated variance in the outcome

---
## ANOVA vs. Regression

* A regression with dummy indicator variables is mathematically identical to ANOVA.

* The F-test in a regression model represents a test of the model's variance against the residual.

* In ANOVA, we can have one or more F-tests where we "batch test" a group of coefficients.
  + Example: Assuming we had multiple variables in our model, we could test the main effect of marital status against the residual rather than examine each individual coefficient.

---
## Disordered Eating Data

Since some of our marital status groups are poorly represented in our dataset (n < 20), we will only examine the differences between the following groups: a) single, b) engaged, c) married, and d) single in a committed relationship.

```{r, echo = F}
status <- c("single", "married", "engaged", "single_committed")
do2 <- do %>% 
  filter(marital_status %in% status) %>% 
  mutate(marital_status = as.factor(as.character(marital_status)))
```

```{r, echo = F, fig.height=4, fig.width=6}
ggplot(do2, aes(x = marital_status, y = BMI, color = marital_status)) +
  geom_jitter() +
  ylim(0, 60) +
  theme_minimal()
```

---
## Partitioning Variance

In regression, we partition our total variance $SS_\text{total}$ into our $SS_\text{model}$ and $SS_\text{error}$.

$SS_\text{model}$ = Deviation of observed value from the predicted value $(Y_{i}-\hat{Y}_i)$.

 $SS_\text{error}$ = Deviation of predicted value from the grand mean $(\hat{Y} - \bar{Y}_i)$.

--

In ANOVA, we apply a similar but slightly different conceptual process.
---
## Partioning Variance in ANOVA

In ANOVA, we separate variance into between-group and within-group variance.

$SS_\text{within}$ = Deviation of observed value from its group mean $(Y_{ik}-\bar{Y}_k)$.

$SS_\text{between}$ = Deviation of group mean from the grand mean $(\bar{Y}_k - \bar{Y})$.

--

$$SS_\text{total} = SS_\text{within} + SS_\text{between}$$
---
## Visualizing ANOVA

```{r, echo = F, fig.height=4, fig.width=6, echo = F}

m <- lm(BMI ~ marital_status, do2)

do2 <- do2 %>% 
  group_by(marital_status) %>% 
  mutate(group_mean = mean(BMI)) %>% 
  ungroup()

do_samp <- do2 %>% 
  group_by(marital_status) %>% 
  sample_n(20)

do_samp$predict <- predict(m, do_samp)

do_samp$jitter_marit <-
  ave(as.numeric(do_samp$marital_status), 
      do_samp$marital_status, 
      FUN = function(x) x + rnorm(length(x), sd = .2))

do_samp <- do_samp %>% 
  mutate(jitmin = min(jitter_marit),
         jitmax = max(jitter_marit))

```

.pull-left[
```{r, fig.height=3, fig.width=5, echo = F}
with_plot <- do_samp %>% 
ggplot(aes(x = jitter_marit, xend = jitter_marit,
                 y = BMI, yend = predict)) +
    geom_segment(color = "green") +
    geom_point() +
    scale_x_continuous("Marital Status", 
                       breaks = c(1, 2, 3, 4),
                       labels = levels(do2$marital_status)) +
  geom_segment(aes(x = jitmin, xend = jitmax, y = predict, yend = predict),
               color = "darkgreen") +
  ylim(0, 40) +
  theme_minimal() +
  ggtitle("Within-Groups Variance")

with_plot
```
This shows the residual variance around the group means. Just like the error term, it is all the remaining variance our predictor can't explain.
]

.pull-right[
```{r, fig.height=3, fig.width=5, echo = F}
btw_plot <- ggplot(do_samp, aes(x = marital_status, y = group_mean)) +
  geom_point(aes(x = as.numeric(marital_status) + 0.5,
                   y = group_mean), color = "darkgreen") +
    geom_segment(aes(x = as.numeric(marital_status),
                   xend = as.numeric(marital_status),
                   y = group_mean,
                   yend = mean(do2$BMI)),
                 linetype = 2) +
  geom_hline(yintercept = mean(do2$BMI), color = "black", linetype = 2) +
      scale_x_continuous("Marital Status", 
                       breaks = c(1, 2, 3, 4),
                       labels = levels(do2$marital_status)) +
  ylim(0, 40) +
  theme_minimal() +
  ggtitle("Between-Groups Variance")

btw_plot
```

This shows the group effect of marital status against the grand mean.
]


---
## ANOVA Test Statistic

When we conduct an ANOVA we are testing the significance of an F statistic using the following formula:

$$\large F = \frac{MS_\text{between}}{MS_\text{within}}$$

The mean squares (MS) of between- and within-group variance is just their Sums of Squares divided by their degrees of freedom.

.pull-left[
$$\large MS_w = \frac{SS_w}{df_w}$$
$$\large df_w = N-G$$

]
.pull-right[
$$\large MS_b = \frac{SS_b}{df_b}$$
$$\large df_b = G-1$$
]
---
## ANOVA Significance Test

The null hypothesis of an ANOVA regards the ratio of between- to within-group variance.

Essentially, we are asking if the mean square variance of the group means around the grand mean is significantly greater than the mean square variance of observations around their group mean.
--
If the between-group variance were much larger than the within-group variance, than the F-statistic would exceed 1.

$$\large F = \frac{MS_\text{between}}{MS_\text{within}} = \frac{4.3}{1.5} = 2.87$$

If the between-group variance is equal to or much smaller than the within-group variance, than our F statistic will be $\le$ 1.

$$\large F = \frac{MS_\text{between}}{MS_\text{within}} = \frac{0.2}{1.5} = 0.13$$

---
## Calculating the F-Statistic

Let's find our F-statistic for our marital status variable. 

```{r, echo = F}
do2$predicted_BMI <- predict(m, do2)
```

#### Within-Group (Residual) Variance $MS_\text{Within}$
```{r}
# total n - number of groups (4)
df_within <- 1079 - 4

sum((do2$BMI - do2$group_mean)^2) / df_within
```

#### Between-Group Variance $MS_\text{Between}$
```{r}
# number of groups (4) - 1
df_btw <- 4-1

sum((mean(do2$BMI) - do2$group_mean)^2) / df_btw
```

---
## Calculating the F-Statistic

$$MS_\text{Between} = 41.76$$
$$MS_\text{Within} = 37.12$$

$$F = \frac{MS_\text{Between}}{MS_\text{Within}} = \frac{41.76}{37.12} = 1.125$$ 
Our F-statistic is 1.125. Now that we see how it is calculated, let's run an ANOVA in R to get our p-value and review the output.

---
## ANOVA Output

Because ANOVA is just a particular method of analyzing variance in GLMs, we can wrap `anova` around our `lm` model.

```{r, highlight.output = 6}
m1 <- lm(BMI ~ marital_status, do2)
car::Anova(m1, type = 3)
```
Here we can see all the information for the residual and marital status we calculated earlier. Only the p-value is new!

--

With a p-value of .34, our F-statistic is not that unusual. Therefore we fail to reject the null hypothesis. The marital status group means do not significantly differ in terms of BMI.


---
class: middle, inverse

# ANCOVA

---
# ANCOVA

* Analysis of covariance (ANCOVA) is an extension of ANOVA.
* Essentially, it is the an ANOVA that controls for the effects of other **continuous** independent variables.
* The null hypothesis is still the same as ANOVA $(\mu_1 = \mu_2 = \mu_K)$.

Sample Research Question: What is the difference in BMI among different marital status groups, **after controlling for age?**

---
## ANCOVA as Regression

Research Question: What is the difference in BMI among different marital status groups, **after controlling for age?**


In a regression equation, categorical variables would still be represented as a dummy-coded variable. We are just adding an additional $\beta$ for our covariate.

--

$$BMI_i = \beta_0 + \beta_1\text{Marital Status}_i + \beta_2\text{Age}_i + \epsilon_i$$

--

$$BMI_i = \beta_0 + \beta_1\text{Single_Commited}_i + \beta_2\text{Engaged}_i + \beta_3\text{Married}_i + \beta_4\text{Age}_i + \epsilon_i$$
--

Now we have to remember to qualify our interpretations given our covariates, or control variables.

$\beta_3$: The predicted difference in BMI between a single male and a married male, after controlling for age.

---
## ANCOVA

Because we are assigning a linear slope to our continuous covariate, it is important to verify our assumption of linearity. 

```{r, echo = F, fig.height = 4, fig.width=6}
ggplot(do, aes(age_year, BMI)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  theme_minimal()


```

---
## ANCOVA Output

Now that we have an additional covariate in the model, we can expect a different residual sums of squares.

```{r, highlight.output = 8,}
m2 <- lm(BMI ~ marital_status + age_year, do2)
car::Anova(m2, type = 3)

```

* In our basic ANOVA, our residual SS was 39,907. Now we see the residual is 39,224. 

* Our variance has been "reorganized" with the addition of `age_year`. 

---
## ANCOVA Output

```{r, highlight.output = c(6, 8)}
car::Anova(m2, type = 3)
```

* Our F-test for marital status still regards the relationship between its SS and the residual SS, but we have now accounted for the variance associated with age.

* Still, our resulting F-value is not significant (p = 0.28), meaning marital status does not significantly account for BMI after controlling for age.

* On the other hand, the main effect of age is significant (p < .001)

---
## Regression Output

We can compare our ANCOVA output to our regression output and see our dummy-coded, "unbatched", analysis of marital status effects. 

```{r, echo = F, highlight.output = c(12, 13, 14), comment = NA}
summary(m2)
```
---

# Variance decomposition

* ANOVA (and ANCOVA) are typically used as methods for analyzing variance associated with group or categorical effects.

* Repeated Measures ANOVA is a similar process, but we define our "groups" as conditions within an individual.
  + e.g., Baseline, Treatment 1, Treatment 2

* ANOVA can be particularly useful for analyzing a regression model's terms, rather than its individual coefficients.



---
End of slides

