---
title: "Multiple Regression"
subtitle: "EDUC 643: General Linear Model I"
author: "TBD"
#date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'uo', 'ki-fonts', 'my_custom.css', 'xaringanthemer.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false

---

```{R, setup, include = F}
library(pacman)
p_load(here, tidyverse, ggplot2, xaringan, knitr, kableExtra, foreign, broom, xaringanthemer, reshape2)

i_am("slides/EDUC643_3_mult_regression.rmd")


extra_css <- list(
  ".red"   = list(color = "red"),
  ".blue"  =list(color = "blue"),
  ".green" = list(color = "#8bb174"),
  ".purple" = list(color = "#6A5ACD"),
  ".red-pink" = list(color= "#e64173"),
  ".grey-light" = list(color= "grey70"),
  ".slate" = list(color="#314f4f"),
  ".small" = list("font-size" = "90%"))

write_extra_css(css = extra_css, outfile = "my_custom.css")

# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 6.75,
  fig.width = 10.5,
  warning = F,
  message = F
)
opts_chunk$set(dev = "svg")
options(device = function(file, width, height) {
  svg(tempfile(), width = width, height = height)
})
options(knitr.table.format = "html")

```
# Roadmap

```{r, echo=F, out.width="90%"}
include_graphics("Roadmap3.jpg")
```
---
# A motivating question

Using real-world data about the dietary habits, health, and self-appraisals of younger males, we aim to answer the following RQ:

- .blue[**Does dietary restraint predict BMI among younger adult males?**]

```{r, echo=F}
do <- read.spss(here::here("data/male_do_eating.sav"), to.data.frame=T) %>% 
  drop_na()
do <- rownames_to_column(do, "id")
do <- select(do, -c(Study_ID))

```
---
# Bivariate relationship
```{r, echo=F}
lm_plot <- ggplot(do, aes(x=EDEQ_restraint, y=BMI)) + 
  geom_point() +
  geom_smooth(method = "lm", se=F) +
  xlab("Dietary restraint index") +
  theme_minimal()

lm_plot
```

---
# Regression results

```{r, echo=F}
fit <- lm(BMI ~ EDEQ_restraint, data=do)
summary(fit)
```

--

.red[**But perhaps there are other features among the participants that also influence their BMI?**]

---
# Goals for the unit

- Articulate the concepts of multiple regression and .red["statistical adjustment"]
- Distinguish between the substantive implications of the terms .red["statistical control"] and .red["statistical adjustment"]
- Estimate the parameters of a multiple regression model
- Visually display the results of multiple regresion models
- State the main effects assumption and what the implication would be if it is violated
- Conduct statistical inference tests of single predictors ( $t$-test) and full model ( $F$-test) in multiple regression
- Decompose the total variance into its component parts (model and residual) and use the $R^{2}$ statistic to describe this decomposition
- Describe problems for regression associated with the phenomenon of multicollinearity
- Use visual schema (e.g., Venn diagrams) to assess regression models for the potential of multicollinearity
- Use statistical results (e.g., correlation matrices or heat maps) to assess regression models for the potential of multicollinearity
- Describe and implement some solutions to multi-collinearity

---
class: middle, inverse

# Statistical adjustment

---
class: middle, inverse

# Inference in multiple regression
---

class: middle, inverse

# Variance decomposition in multiple regression

---
# Bivariate v. multiple regression

|                     |  Bivariate regression       |   Multiple regression
|----------------------------------------------------------------------------
| Model specification | $\hat{Y}=\hat{\beta_{0}} + \hat{\beta_{1}}\mathbf{X}_{1}$ | $\hat{Y}=\hat{\beta_{0}} + \hat{\beta_{1}}\mathbf{X}_{1} + \hat{\beta_{2}}\mathbf{X}_{2} + \cdots \hat{\beta_{k}}\mathbf{X}_{k}$
| Interpretation of $\hat{\beta_{0}}$ | Predicted value of Y when X=0 | Predicted value of Y when .red[all] Xs = 0
| Interpretation of $\hat{\beta_{1}}$ | Difference in Y per 1 unit of X | Difference in Y per 1 unit difference in $X_{1}$, adjusting for $X_{2} \cdots X_{k}$
| Graphical representation | Fitted line     | Fitted plane in 3D (with two Xs) <br> Plot with prototypical lines
| Residuals   | Distance between observation and fitted .red-pink[line] | Distance between line and fitted .red-pink[plane]
| Inference: $t$-tests | $H_{0} = \beta_{1} = 0$ <br> Is there a relationship between X and Y in pop? | $H_{0} = \beta_{1} = 0$ <br> Adjusting for  $X_{2} \cdots X_{k}$ is there a relationship between $X_{1}$ and Y in the population? <br> Repeat for each X

---
# Bivariate v. multiple regression

|                     |  Bivariate regression       |   Multiple regression
|----------------------------------------------------------------------------
| Inference: $F$-test | $H_{0} = \beta_{1} = 0$ <br> same result as $t$-test |  $H_{0} = \textrm{all } \beta_{1} = 0$ <br> Does *any* predictor have a relationship with Y in the population?
| $R^{2}$ | $\frac{Regress SS}{Total SS}$ <br> % of variation in Y explained by X | $\frac{Regress SS}{Total SS}$ <br> % of variation in Y explained by $X_{1} \cdots X_{k}$ 
| Regression assumptions | See prior unit          | Same as bivariate, but .red[**at each combination of the Xs**]. <br> Main effects assumption





---
class: middle, inverse

# Multi-collinearity

---
# Power of multiple regression

Multiple regression can be a powerful tool to adjust for sample differences that depend on a variable other than the one in which we are interested and focus on the key question we have.

Take this example of a theoretical relationship between height and reading ability:
```{r, echo=F, fig.height=5}
set.seed(1234)
h <- rep(40, 1000)
g <- c("2nd", "3rd", "4th", "5th")    
grade <- rep(g, 250)
reading <- cbind.data.frame(grade, h)

reading <- reading %>% mutate(height = case_when(grade=="2nd" ~ h,
                                                        grade=="3rd" ~ h + 3,
                                                        grade=="4th" ~ h + 5,
                                                        grade=="5th" ~ h + 7))

reading <- reading %>% mutate(height = height + rnorm(length(height), 0, 4.5))

reading <- reading %>% mutate(read = rnorm(length(h)))

reading <- reading %>% mutate(read = case_when(grade=="2nd" ~ read,
                                               grade=="3rd" ~ read + .5,
                                               grade=="4th" ~ read + .8,
                                               grade=="5th" ~ read + 1.1))

reading <- reading %>% group_by(grade) %>% mutate(read = read + rnorm(length(read)))

rdplot <- ggplot(data=reading, aes(height, read)) + 
              geom_point() +
              theme_minimal()

rdplot  

```
---
# Power of multiple regression

Multiple regression can be a powerful tool to adjust for sample differences that depend on a variable other than the one in which we are interested and focus on the key question we have.

Take this example of a theoretical relationship between height and reading ability:
```{r, echo=F, fig.height=5}
rdplot +
  geom_smooth(method='lm', se=F) +
  annotate('text', label = "slope = 0.05 (0.01)", x = 28, y = 0.3, color = "blue", size = 4) +
  theme_minimal()

```

--

.blue[*Do we really believe this or are there statistical adjustments we can make to reveal the true nature of the relationship?*]

---
# Power of multiple regression
Multiple regression can be a powerful tool to adjust for sample differences that depend on a variable other than the one in which we are interested and focus on the key question we have.

Take this example of a theoretical relationship between height and reading ability:
```{r, echo=F, fig.height=5}
ggplot(data=reading, aes(height, read, col=grade)) + 
  geom_point() +
  geom_smooth(method='lm', se=F) +
  theme_minimal()
```
---
# Power of multiple regression
Formally testing this:

```{r, echo=T}
summary(lm(read ~ height + grade, data=reading))
```
---
# Limits of multiple regression

However, sometimes, multiple regression cannot solve the problem if predictors are "too highly" correlated. For example, if women and men have unequal access to jobs of different status, adjusting for job status will not recover the relationship between gender and wages.<sup>[1]</sup>

```{r, echo=F, fig.height=4.5}
w <- seq(50001, 100000)
g <- c("Women", "Men")
gend <- rep(g, 25000)
s <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
stat <- rep(s, 5000)

gender <- as.data.frame(gend)
wages <- as.data.frame(w)
status <- as.data.frame(stat)

discr <- arrange(gender, desc(gend)) %>% cbind(., wages)
discr <- arrange(status, stat) %>% cbind(., discr)

discr <- discr %>% group_by(stat) %>% mutate(wage = w + rnorm(length(w), 0, 1000))
discr <- discr %>% mutate(status = stat + rnorm(length(stat), 0, 0.05))

ggplot(data=discr, aes(status, wage)) +
  geom_jitter() +
  theme_minimal()
```
.footnote[[1] This was a problem many researchers identified in [Google's efforts] (https://www.npr.org/2019/03/05/700288695/google-pay-study-finds-its-underpaying-men-for-some-jobs) to document pay disparities in 2019.]
---
# Limits of multiple regression

However, sometimes, multiple regression cannot solve the problem if predictors are "too highly" correlated. For example, if women and men have unequal access to jobs of different status, adjusting for job status will not recover the relationship between gender and wages.
```{r, echo=F, fig.height=4.5}
ggplot(data=discr, aes(status, wage, col=gend)) +
  geom_jitter() +
  theme_minimal()
```

---
# Limits of multiple regression

On average, women have lower wages than men:
```{r, echo=T}
tidy(lm(wage ~ gend, data=discr))
```

On average, women are in lower status jobs than men:
```{r, echo=T}
tidy(lm(wage ~ status, data=discr))
```
---
# Limits of multiple regression

However, once we adjust for job status, there is no wage differential, on average in the population, between men and women:
```{r, echo=T}
tidy(lm(wage ~ gend + status, data=discr))
```

--

If two predictors are "too highly" correlated, we can't adjust for one to evaluate the effects of the other. This is known as .red[**multicollinearity**].

---
# Multicollinearity

Multicollinearity occurs when predictor variables are highly related to each other.

This can be a simple relationship, such as when two of our predictors are strongly related to one another. This is usually straightforward to recognize, interpret, and correct for.

Sometimes multicollinearity is difficult to detect, such as when our variable of interest (e.g., $X_{1}$) is not strongly correlated with any one $X_{k}$, but the combination of the $X$s is a strong predictor of $X_{1}$.

--

### Multicollinearity biases our regression estimates and increases the standard errors of our regression coefficients.


---
# Venn diagrams of collinearity

---
# Estimates of collinearity in R

```{r, echo=F}
cordat <- do %>% select(EDS_total, OE_frequency, BMI, MBAS_muscularity, MBAS_height, DMS_mean) %>%
  rename(EDS_tot = EDS_total,
         OE_freq = OE_frequency,
         MBAS_musc = MBAS_muscularity)
```
```{r, echo=T}
cormat <- round(cor(cordat),3)
cormat[lower.tri(cormat)] <- NA
print(cormat)
```

---
# Visual heatmap

```{r, echo=F}
melt_corm <- melt(cormat, na.rm=T)
ggplot(data = melt_corm, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()

```
---
# Correlation and collinearity

Perfect collinearity never happens (except in the instance of a duplicated variable). There are degrees of multicollinearity. 

### More multicollinearity = more problematic model.

--

In practice, when we detect problems with collinearity, what we are really detecting is strongly correlated predictors. 

Note that in the example of height and grade, once we partial out grade, there is no relationship between height and reading. However, after partialling out height, there is still a relationship between grade and reading. This is because there is still variation in grade at each value of height.  

.red[**Don't abuse the term collinear!**]

---
# Putting multicollinearity together

1. **Statistical adjustments can help recover the "true" relationship in your data that is obscured by confounding variables.**
2. **When two variables are highly correlated, it may be impossible to adjust for one**
 - This is known as the problem of .red[**multicollinearity**] (*though the term is not quite right!*)
3. **Graphical representations (such as Venn diagrams) can help you conceptualize the potential for multicollinearity**
4. **Use correlation matrices to detect for the phenomenon of highly correlated variables**
 - Consider visual representations to detect patterns more easily
5. **Solutions to multicollinearity**:
 - Increase sample size, remove a variable, create a composite or factor score (more to come in EDUC 645!)
 
---
class: middle, inverse
# Synthesis and wrap-up
