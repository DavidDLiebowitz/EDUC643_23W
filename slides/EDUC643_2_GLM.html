<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>The General Linear Model (GLM)</title>
    <meta charset="utf-8" />
    <meta name="author" content="David D. Liebowitz" />
    <script src="EDUC643_2_GLM_files/header-attrs-2.14.3/header-attrs.js"></script>
    <link href="EDUC643_2_GLM_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="EDUC643_2_GLM_files/remark-css-0.0.1/uo.css" rel="stylesheet" />
    <link href="EDUC643_2_GLM_files/remark-css-0.0.1/ki-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my_custom.css" type="text/css" />
    <link rel="stylesheet" href="xaringanthemer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# The General Linear Model (GLM)
]
.subtitle[
## EDUC 643: Applied Statistics in Education and the Human Services II
]
.author[
### David D. Liebowitz
]

---



# Roadmap

&lt;img src="Roadmap1.jpg" width="90%" style="display: block; margin: auto;" /&gt;


---
# Goals for the unit

.gray[
- Characterize a bivariate relationship along five dimensions (direction, linearity, outliers, strength and magnitude)
]

- Describe how statistical models differ from deterministic models
- Mathematically represent the population model and interpret its deterministic and stochastic components
- Formulate a linear regression model to hypothesize a population relationship
- Estimated a fitted regression line using Ordinary-Least Squares regression
- Describe residuals and how they can describe the degree of our OLS model fit

.gray[
- Explain `\(R^{2}\)`, both in terms of what it tells us and what it does not
- Conduct an inference test for a regression coefficient and our regression model
- Calculate a correlation coefficient `\((r)\)` and describe its relationship to `\(R^{2}\)`
- Distinguish between research designs that permit correlational associations and those that permit causal inferences
]

---
class: middle, inverse


---
# A motivating question

Researchers (including two from the .green[University of Oregon]), [Nichole Kelly, Elizabeth Cotter and Claire Guidinger (2018)](https://doi.org/10.1016/j.eatbeh.2018.07.003), set out to understand the extent to which young men who exhibit overeating behaviors have weight-related medical and psychological challenges.

&lt;img src="kelly_etal.png" width="955" height="85%" style="display: block; margin: auto;" /&gt;

Using real-world data (generously provided by Nichole Kelly) about the dietary habits, health, and self-appraisals of males 18-30, we are going to attempt to answer a similar question. 

However, before answering this question, we are going to explore **the relationship between dietary restraint behaviors** (self-reports on the extent to which participants consciously restricted/controlled their food intake) **and body-mass index (BMI).**


---
# Reading in the data

```r
do &lt;- read_spss(here("data/male_do_eating.sav")) %&gt;% 
    select(Study_ID, BMI, age_year, income_group, OE_frequency, 
         EDEQ_restraint, DERS_total, DMS_mean, EDS_total, 
          MBAS_muscularity, MBAS_height, SATAQ_total) %&gt;%
        drop_na()
# note: we are focusing only on those cases that had complete records  
#       for all these variables
```

---
# Bivariate relationships

Five ways to characterize them

.pull-left[
- Direction
- Linearity
- Outliers
- Strength
- Magnitude
]

.pull-right[
&lt;img src="EDUC643_2_GLM_files/figure-html/unnamed-chunk-4-1.svg" style="display: block; margin: auto;" /&gt;
]
---
# A line through our cloud

Notice that in the previous slide, we added a line running through our data

&lt;img src="EDUC643_2_GLM_files/figure-html/unnamed-chunk-5-1.svg" style="display: block; margin: auto;" /&gt;
That line is defined by the intercept (value `\(Y\)` takes when `\(X=0\)`) and the slope (the difference in `\(Y\)` per 1 unit difference in `\(X\)`)
&gt; `\(Y = intercept + slope*X\)` (you may have seen this in HS as Y = mX + b)

&gt; We could think of this relationship, therefore, as `\(BMI = slope*EDEQrestraint + intercept\)` ... .purple[but that's not quite right]

---
# Mathematical representations

In addition to visual representations, we can borrow from mathematical models to construct a statistical relationship between variables. However, these are not identical.

.pull-left[
**Mathematical models**

- Are deterministic
- The area of any square is always `\(s^{2}\)`
- Once we know the rule, we can use it to fit the model to data perfectly
]

.pull-right[
**Statistical models**

- Include individual variation
- Other systematic components exist that are either not measured or observable
- Outcome = Systematic component + residual
]

--

.blue[*What is wrong about describing the relationship between BMI and eating restraint as we did on the previous slide?*]

---
# Statistical model

In order to develop our statistical model (inspired by a deterministic mathematical model), we need to first determine our model's functional form.

&lt;img src="EDUC643_2_GLM_files/figure-html/unnamed-chunk-6-1.svg" style="display: block; margin: auto;" /&gt;

--

.blue[All of the models we will focus on in this course and next are linear. Why?]

---
# Linear models

### Why are straight lines so popular in statistics?
1. Mathematical simplicity: straight lines are one of the simplest (and consistent) ways of characterizing relationships
2. Actual linearity: many relationships are best characterized linearly

### What if my data isn't related in linear ways
1. Transformations: we will learn how to use these later to fit linear models to curvilinear data
2. Limited ranges of X may yield linearity: most things are "locally linear"

.red[**In fact, linear modeling is so tractable that this is what we will spend the entire course on! And the next one (EDUC 645)!**]

--

We'll learn about lots of different modeling approaches. .red[**They are all part of the same family of models, known as the General Linear Model (GLM)**]. We will learn more about this GLM in Unit 4.

---
# Linear reqression equation

Okay, so once we have selected our model's functional form (which for now and the foreseeable future is going to be "linear"), we can move on to a mathematical representation. In this case, we are going to specify a .red[**linear regression**] model.

A linear regression equation borrows the mathematical framework for a line to summarize this relationship.
`$$BMI = \beta_0 + \beta_1(DietaryRestraint)$$`
---
# Regression equations

The regression line describes the mean value of Y for each possible "input" value of X.

`$$BMI = \beta_0 + \beta_1(DietaryRestraint)$$`

--
Our data already provides us with observations of BMI and dietary restraint:


```
#&gt; # A tibble: 6 x 2
#&gt;   EDEQ_restraint   BMI
#&gt;            &lt;dbl&gt; &lt;dbl&gt;
#&gt; 1            0    23.3
#&gt; 2            1.2  16.6
#&gt; 3            0.4  18.7
#&gt; 4            0.4  15.4
#&gt; 5            0    20.2
#&gt; 6            1.2  26.3
```

--
So, modelling requires us to find the best intercept `\((\beta_0)\)` and slope `\((\beta_1)\)` to represent the relationship.

---
# Regression equation components

`$$BMI = \color{orange}{(\beta_0)} + \color{purple}{(\beta_1)}(DietaryRestraint)$$`

&lt;span style = "color:orange"&gt; Intercept `\((\beta_0)\)`: &lt;/span&gt; Predicted outcome when X is equal to 0.

--

&lt;span style = "color:purple"&gt; Slope `\((\beta_1)\)`: &lt;/span&gt; Predicted increase in the outcome for every one unit increase in X.

--

**Write out the appropriate regression equation for a line that has an intercept of 20 and Dietary Restraint slope of 1.5.**

---
# Regression equation components

&lt;span style = "color:orange"&gt; Intercept `\((\beta_0)\)`: &lt;/span&gt; Predicted outcome when X is equal to 0.

&lt;span style = "color:purple"&gt; Slope `\((\beta_1)\)`: &lt;/span&gt; Predicted increase in the outcome for every one unit increase in X.


`$$BMI = \color{orange}{20} + \color{purple}{1.5}(DietaryRestraint)$$`
**What is the expected BMI value for a Dietary Restraint rating of 2?**
---
# Regression equation components

&lt;span style = "color:orange"&gt; Intercept `\((\beta_0)\)`: &lt;/span&gt; Predicted outcome when X is equal to 0.

&lt;span style = "color:purple"&gt; Slope `\((\beta_1)\)`: &lt;/span&gt; Predicted increase in the outcome for every one unit increase in X.



`$$BMI = \color{orange}{20} + \color{purple}{1.5}(2) = 20 + 3 = 23$$`
**For a Dietary Restraint rating of 2, the predicted BMI value is 23.**
---
# Error/residual term
"No model is perfect, but some are useful." - George Box

&lt;img src="EDUC643_2_GLM_files/figure-html/unnamed-chunk-8-1.svg" style="display: block; margin: auto;" /&gt;

Is Dietary Restraint a perfect predictor of BMI? 

--

.red[NO!]

---
# Omitted variables
A "perfect" regression equation would probably include a lot more variables:

`$$BMI = \beta_0 + \beta_1(\text{Dietary Restraint}) + \beta_2(\text{Meal Frequency}) + \beta_3(\text{Nutritional Habits})... \beta_\infty$$`
Many of these might not even be measurable!

--

Omitted variables are not a problem in themselves. Our estimates of the relationship between two (or more) variables might still be unbiased (i.e., accurately describe the nature of the relationship in the population), but they may be less precise because we have not explained all of the variation.

--

However, omitted variables often introduce bias into our estimates of the relationship. More on the problem of omitted variable bias later.

---
# Residual/Error term

In a regression model, all the variability that we couldn't explain with our predictor is condensed into a &lt;span style="color:green"&gt; residual term `\(\varepsilon\)` &lt;/span&gt;.

&lt;img src="EDUC643_2_GLM_files/figure-html/unnamed-chunk-9-1.svg" style="display: block; margin: auto;" /&gt;

`$$BMI = \beta_0 + \beta_1(DietaryRestraint) + \color{green}{\varepsilon}$$`
---
# The regression model

So there it is! Our full population regression model:

`$$Y = \color{blue}{\beta_{0} + \beta_{1} X} + \color{green}{\varepsilon}$$`
&lt;div style= "text-align:center"&gt; Outcome = .blue[Systematic component] + .green[residual]&lt;sup&gt;1&lt;/sup&gt;&lt;/div&gt;

&gt; `\(Y\)`: our outcome &lt;br&gt;
`\(\color{blue}{\beta_{0}}\)` and `\(\color{blue}{\beta_{1}}\)`: our population parameters and regression coefficients to be estimated &lt;br&gt;
`\(\color{green}{\varepsilon}\)`: our error/residual ( `\(\varepsilon\)` is a fancy way of writing the Greek letter "epsilon")

Also written as:

`$$Y_{i} = \beta_{0} + \beta_{1} X_{i} + \varepsilon_{i}$$`
where we use the subscript `\(i\)` to emphasize that the model estimates the outcome for each of the `\(i\)` units (students, schools, patients, etc.).

.footnote[[1] Sometimes also called deterministic and stochastic components.]
---
# A fitted model

A **fitted model** takes our popualtion model and uses our observed data to derive estimates for the population. 
`$$\hat{Y_{i}} = \hat{\beta_{0}} + \hat{\beta_{1}} X_{i}$$`
We denote that we are generating estimates with "hats" above the estimated coefficients. Note that there is ***no error term*** in our fitted model. 

---
# Residuals

For any observation, the residual is the difference between the observed and predicted value.

`$$\varepsilon_i = Y_i - \hat{Y_i}$$`


---
# Ordinary Least Squares (OLS)

An OLS-fitted regression would go through the "center" of the data, finding the best intercept and slope to minimize the total distance between all of the residual and itself.


&lt;img src="EDUC643_2_GLM_files/figure-html/unnamed-chunk-10-1.svg" style="display: block; margin: auto;" /&gt;

--

For some observations we under-predict, for others we over-predict. But, across the full sample, no matter where we put our line, the residuals will always sum to zero. .blue[So how do we calculate the "best" line?]

---
# Sum of the squared residuals

For any observation, the residual is the difference between the observed and predicted value.

`$$\varepsilon_i = Y_i - \hat{Y_i}$$`

Squaring the residual terms `\((\varepsilon_i^2)\)` allows us to treat negative and positive deviations equally, and puts a greater penalty on larger deviations.


```
# A tibble: 6 x 4
    BMI predicted_BMI residual residual_sq
  &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;
1  23.3          23.9    -0.62        0.38
2  16.6          25.2    -8.56       73.3 
3  18.8          24.4    -5.61       31.5 
4  15.4          24.4    -8.91       79.4 
5  20.2          23.9    -3.72       13.8 
6  26.3          25.2     1.1         1.21
```
--

The **sum of squares** is the sum of all our squared residuals:

`$$\Large \Sigma(\varepsilon_i)^2$$`

---
# Ordinary Least Squares (OLS)

An OLS-fitted regression finds the best intercept and slope values to **minimize the sum of squared residuals**.

.pull-left[
&lt;img src="EDUC643_2_GLM_files/figure-html/unnamed-chunk-12-1.svg" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="EDUC643_2_GLM_files/figure-html/unnamed-chunk-13-1.svg" style="display: block; margin: auto;" /&gt;

]

Which regression line appears to be a better fit?

---
# Ordinary Least Squares (OLS)

An OLS-fitted regression finds the best intercept and slope values to minimize the sum of squared residuals.

.pull-left[
&lt;img src="EDUC643_2_GLM_files/figure-html/unnamed-chunk-14-1.svg" style="display: block; margin: auto;" /&gt;
`\(\bf{\Sigma(e_i)^2 = 67314.93}\)`

]

.pull-right[
&lt;img src="EDUC643_2_GLM_files/figure-html/unnamed-chunk-15-1.svg" style="display: block; margin: auto;" /&gt;
`\(\Sigma(e_i)^2 = 100859.80\)`
]

Using the OLS method, the regression line on the left is a better fit (67,314 &lt; 100,859).

---
# Sum of Squared Residuals (SSR)

SSR is a grander concept than just model predictions. It is a way of thinking about variability.

--

Our outcome's variability is simply the sum of squared deviations from its mean.

`$$SSR_\text{BMI} = \sum{(Y_i - \bar{Y})^2}$$`
&lt;img src="EDUC643_2_GLM_files/figure-html/unnamed-chunk-16-1.svg" style="display: block; margin: auto;" /&gt;
---
Get a feel for trying to minimize the *sum of the square of the residuals*
&lt;iframe src="https://daviddl.shinyapps.io/line_ss/?showcase=0" width="100%" height="550px" data-external="1"&gt;&lt;/iframe&gt;
---

# Partitioning variance

The goal of regression is to account for some of this variability with our model's predictors.

We can partition the outcome's total variance ( `\(SS_{BMI}\)`) into:
* Model-accounted variance ( `\(SS_\text{Model}\)`)
* Residual variance ( `\(SS_\text{Residual}\)`)

`$$SS_\text{BMI} = SS_\text{Model} + SS_\text{Residual}$$`
--

Remember our goal with OLS regression is to find the model coefficients that minimize `\(SS_\text{Residual}\)`.


---
# Partitioning variance
`$$\text{If } SS_\text{BMI} = \color{purple}{SS_\text{Model}} + {SS_\text{Residual}}$$`

`\(\color{purple}{SS_\text{Model}}\)` is the sum of squared deviations between the model predicted values `\((\hat{Y})\)` and the mean `\((\bar{Y})\)`.

&lt;img src="EDUC643_2_GLM_files/figure-html/unnamed-chunk-18-1.svg" style="display: block; margin: auto;" /&gt;
`$$\color{purple}{SS_\text{Model} = \sum{(\hat{Y} - \bar{Y})^2}}$$`
---
# Partitioning variance
`$$\text{If } SS_\text{BMI} = \color{purple}{SS_\text{Model}} + \color{green}{SS_\text{Residual}}$$`

`\(\color{green}{SS_\text{Residual}}\)` is the sum of squared deviations between the model predicted values `\((\hat{Y})\)` and the observed values `\((Y)\)`.

&lt;img src="EDUC643_2_GLM_files/figure-html/unnamed-chunk-19-1.svg" style="display: block; margin: auto;" /&gt;
`$$\color{green}{SS_\text{Model} = \sum{(Y - \hat{Y})^2}}$$`

---
# `\(R^2\)`

Partitioning variance can be useful for model evaluation.

`$$SS_\text{BMI} = SS_\text{Model} + SS_\text{Residual}$$`
`\(R^2\)` ("R-squared") is the proportion of variance in the outcome our model accounts for.

`$$R^2 = \frac{SS_\text{Model}}{SS_\text{BMI}}$$`
--
For example, if `\(R^2 = 0.30\)`, then our model accounts for 30% of our sample's variance in BMI.

--
More on this soon.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
