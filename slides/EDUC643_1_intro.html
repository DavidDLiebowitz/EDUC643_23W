<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Intro and regression refresher</title>
    <meta charset="utf-8" />
    <meta name="author" content="TBD" />
    <script src="EDUC643_1_intro_files/header-attrs-2.8/header-attrs.js"></script>
    <link href="EDUC643_1_intro_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="new.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Intro and regression refresher
## EDUC 643: General Linear Model I
### TBD

---



# Roadmap
&lt;font size = "3"&gt;
.pull-left[
1. A refresher on foundations of the General Linear Model
 - Review of bivariate regression
 - Coefficient and model-level inference
 - Correlation and causality
 
2. Assumptions and diagnostics
 - Residuals, studentized, standardized
 - Normality of residuals, homoscedasticity, linearity and independence
 - Diagnostics

3. Multiple regression
 - Concept of statistical adjustment (control)
 - Variance decomposition in MR
 - Measuring effect size in MR
 - Multi-collinearity
]

.pull-right[
4. Categorical predictors
 - Coding dummy variables
 - ANOVA as a special regression model with one categorical predictor
 - Adding a continuous predictor, ANCOVA as a special regression model with a categorical predictor and one or many continuous predictors
 - Variance decomposition when categorical predictors are in the model

5. Interactions and non-linearity
 - Intro to the concept of interaction in MR models
 - Interpreting the interaction between a categorical predictor and continuous predictor
 - Interpreting the interaction between two continuous predictors

6. Model building
]
&lt;/font&gt;

---
class: middle, inverse

# Bivariate regression

---

## A motivating question

Using real-world data about the dietary habits, health, and self-appraisals of younger males, we aim to answer the following RQ:
* Does dietary restraint predict BMI among younger adult males?


```r
do &lt;- read.spss(here::here("data/male_do_eating.sav"), to.data.frame=T) %&gt;% 
  select(Study_ID, EDEQ_restraint, BMI) %&gt;% 
  drop_na()
summary(do$EDEQ_restraint)
```

```
#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#&gt;   0.000   0.200   1.200   1.387   2.200   6.000
```

```r
summary(do$BMI)
```

```
#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#&gt;   7.891  21.520  24.364  25.376  28.284  57.204
```

---
## Bivariate relationships

We are interested in the *relationship* between dietary restraint and BMI. Statistically, the relationship is the same regardless of which variable is the outcome.

.pull-left[
&lt;img src="EDUC643_1_intro_files/figure-html/unnamed-chunk-2-1.png" style="display: block; margin: auto;" /&gt;
]
.pull-right[
&lt;img src="EDUC643_1_intro_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;
]

---
## Regression Equations

For theoretical reasons, we are choosing to regress BMI (DV) on dietary restraint (IV). 

&lt;img src="EDUC643_1_intro_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;

A linear regression equation borrows the mathematical framework for a line to summarize this relationship.
`$$\text{BMI} = \beta_0 + \beta_1(\text{Dietary_Restraint})$$`
---
## Regression Equations

The regression line describes the mean value of Y for each possible "input" value of X.

`$$\text{BMI} = \beta_0 + \beta_1(\text{Dietary_Restraint})$$`

--
Our data already provides us with observations of BMI and dietary restraint:


```
#&gt;   EDEQ_restraint      BMI
#&gt; 1            0.0 23.32485
#&gt; 2            1.2 16.62282
#&gt; 3            0.4 18.74667
#&gt; 4            0.4 15.44678
#&gt; 5            0.0 20.22119
#&gt; 6            1.2 26.28313
```

--
So, modelling requires us to find the best intercept `\((\beta_0)\)` and slope `\((\beta_1)\)` to represent the relationship.

---
## Components of a regression equation

`$$BMI = \color{orange}{(\beta_0)} + \color{purple}{(\beta_1)}(\text{Dietary Restraint})$$`

&lt;span style = "color:orange"&gt; Intercept `\((\beta_0)\)`: &lt;/span&gt; Predicted outcome when X is equal to 0.

--

&lt;span style = "color:purple"&gt; Slope `\((\beta_1)\)`: &lt;/span&gt; Predicted increase in the outcome for every one unit increase in X.

--

**Write out the appropriate regression equation for a line that has an intercept of 20 and Dietary Restraint slope of 1.5.**

---
## Components of a regression equation

&lt;span style = "color:orange"&gt; Intercept `\((\beta_0)\)`: &lt;/span&gt; Predicted outcome when X is equal to 0.

&lt;span style = "color:purple"&gt; Slope `\((\beta_1)\)`: &lt;/span&gt; Predicted increase in the outcome for every one unit increase in X.


`$$BMI = \color{orange}{20} + \color{purple}{1.5}(\text{Dietary Restraint})$$`
**What is the expected BMI value for a Dietary Restraint rating of 2?**
---
## Components of a regression equation

&lt;span style = "color:orange"&gt; Intercept `\((\beta_0)\)`: &lt;/span&gt; Predicted outcome when X is equal to 0.

&lt;span style = "color:purple"&gt; Slope `\((\beta_1)\)`: &lt;/span&gt; Predicted increase in the outcome for every one unit increase in X.



`$$BMI = \color{orange}{20} + \color{purple}{1.5}(2) = 20 + 3 = 23$$`
**For a Dietary Restraint rating of 2, the predicted BMI value is 23.**
---
## Error/Residual Term
"No model is perfect, but some are useful." - George Box

&lt;img src="EDUC643_1_intro_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;

Is Dietary Restraint a perfect predictor of BMI?

---
## Error/Residual Term
"No model is perfect, but some are useful." - George Box

&lt;img src="EDUC643_1_intro_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;

Is Dietary Restraint a perfect predictor of BMI? - NO! 

---
## Unmeasured Variables
A "perfect" regression equation would probably include a lot more variables:

`$$BMI = \beta_0 + \beta_1(\text{Dietary Restraint}) + \beta_2(\text{Meal Frequency}) + \beta_3(\text{Nutritional Habits})... \beta_\infty$$`
Many of these might not even be measurable!

---
## Residual/Error Term

In a regression model, all the variability that we couldn't explain with our predictor is condensed into a &lt;span style="color:green"&gt; residual term `\((e_1)\)` &lt;/span&gt;.

&lt;img src="EDUC643_1_intro_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;

`$$BMI = \beta_0 + \beta_1(\text{Dietary Restraint}) + \color{green}{e_1}$$`
---
## Sum of Squares

For any observation, the residual is the difference between the observed and predicted value.

`$$e_i = Y_i - \hat{Y_i}$$`

--

Squaring the residual terms `\((e_i^2)\)` allows us to treat negative and positive deviations equally, and puts a greater penalty on larger deviations.


```
    BMI predicted_BMI residual residual_sq
1 23.32         23.94    -0.61        0.38
2 16.62         25.18    -8.56       73.27
3 18.75         24.35    -5.61       31.43
4 15.45         24.35    -8.91       79.32
5 20.22         23.94    -3.72       13.82
6 26.28         25.18     1.10        1.21
```
--

The **sum of squares** is the sum of all our squared residuals:

`$$\Large \Sigma(e_i)^2$$`

---
## Ordinary Least Squares (OLS) Regression

An OLS-fitted regression finds the best intercept and slope values to minimize the sum of squared residuals.

.pull-left[
&lt;img src="EDUC643_1_intro_files/figure-html/unnamed-chunk-10-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="EDUC643_1_intro_files/figure-html/unnamed-chunk-11-1.png" style="display: block; margin: auto;" /&gt;

]

Which regression line appears to be a better fit?

---
## Ordinary Least Squares (OLS) Regression

An OLS-fitted regression finds the best intercept and slope values to minimize the sum of squared residuals.

.pull-left[
&lt;img src="EDUC643_1_intro_files/figure-html/unnamed-chunk-12-1.png" style="display: block; margin: auto;" /&gt;
`\(\bf{\Sigma(e_i)^2 = 67314.93}\)`

]

.pull-right[
&lt;img src="EDUC643_1_intro_files/figure-html/unnamed-chunk-13-1.png" style="display: block; margin: auto;" /&gt;
`\(\Sigma(e_i)^2 = 100859.80\)`
]

Using the OLS method, the regression line on the left is a better fit (67,314 &lt; 100,859).

---
## Sum of Squared Residuals (SSR)

SSR is a grander concept than just model predictions. It is a way of thinking about variability.

--

Our outcome's variability is simply the sum of squared deviations from its mean.

`$$SSR_\text{BMI} = \sum{(Y_i - \bar{Y})^2}$$`
&lt;img src="EDUC643_1_intro_files/figure-html/unnamed-chunk-14-1.png" style="display: block; margin: auto;" /&gt;
---
## Partitioning Variance

The goal of regression is to account for some of this variability with our model's predictors.

We can partition the outcome's variance into:
* Model-accounted variance
* Residual variance

`$$SS_\text{BMI} = SS_\text{Model} + SS_\text{Residual}$$`
--

Remember our goal with OLS regression is to find the model coefficients that minimize `\(SS_\text{Residual}\)`.


---
# Partitioning Variance
`$$\text{If } SS_\text{BMI} = \color{purple}{SS_\text{Model}} + {SS_\text{Residual}}$$`

`\(\color{purple}{SS_\text{Model}}\)` is the sum of squared deviations between the model predicted values `\((\hat{Y})\)` and the mean `\((\bar{Y})\)`.

&lt;img src="EDUC643_1_intro_files/figure-html/unnamed-chunk-15-1.png" style="display: block; margin: auto;" /&gt;
`$$\color{purple}{SS_\text{Model} = \sum{(\hat{Y} - \bar{Y})^2}}$$`
---
# Partitioning Variance
`$$\text{If } SS_\text{BMI} = \color{purple}{SS_\text{Model}} + \color{green}{SS_\text{Residual}}$$`

`\(\color{green}{SS_\text{Residual}}\)` is the sum of squared deviations between the model predicted values `\((\hat{Y})\)` and the observed values `\((Y)\)`.

&lt;img src="EDUC643_1_intro_files/figure-html/unnamed-chunk-16-1.png" style="display: block; margin: auto;" /&gt;
`$$\color{green}{SS_\text{Model} = \sum{(Y - \hat{Y})^2}}$$`

---
## R-Squared

Partitioning variance can be useful for model evaluation.

`$$SS_\text{BMI} = SS_\text{Model} + SS_\text{Residual}$$`
`\(R^2\)` is the proportion of variance in the outcome our model accounts for.

`$$R^2 = \frac{SS_\text{Model}}{SS_\text{BMI}}$$`
--
For example, if `\(R^2 = 0.30\)`, then our model accounts for 30% of our sample's variance in BMI.

--
More on this soon.

---
## Evaluating a regression

Let's try fitting our regression in R.


```r
fit &lt;- lm(BMI ~ EDEQ_restraint, do)
summary(fit)
```

```
#&gt; 
#&gt; Call:
#&gt; lm(formula = BMI ~ EDEQ_restraint, data = do)
#&gt; 
#&gt; Residuals:
#&gt;     Min      1Q  Median      3Q     Max 
#&gt; -19.064  -3.962  -0.928   2.685  33.266 
#&gt; 
#&gt; Coefficients:
#&gt;                Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept)     23.9383     0.2631  90.973  &lt; 2e-16 ***
#&gt; EDEQ_restraint   1.0369     0.1358   7.638  4.8e-14 ***
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 6.081 on 1092 degrees of freedom
#&gt; Multiple R-squared:  0.05072,	Adjusted R-squared:  0.04985 
#&gt; F-statistic: 58.34 on 1 and 1092 DF,  p-value: 4.799e-14
```


---
## Evaluating a regression - Coefficients

Here we can find our intercept and slope coefficients for our linear regression.


```
...
Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
*(Intercept)     23.9383     0.2631  90.973  &lt; 2e-16 ***
*EDEQ_restraint   1.0369     0.1358   7.638  4.8e-14 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.081 on 1092 degrees of freedom
Multiple R-squared:  0.05072,	Adjusted R-squared:  0.04985 
F-statistic: 58.34 on 1 and 1092 DF,  p-value: 4.799e-14
...
```

**The predicted BMI for a young male with a dietary restraint rating of 0 is 23.94 `\((\beta_0)\)`.**

--

**For each one unit increase in dietary restraint, there is an expected increase of 1.04 `\((\beta_1)\)` in BMI.**

---
## Evaluating a regression - Residuals

Here is our summary of the residual error.


```
...
Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     23.9383     0.2631  90.973  &lt; 2e-16 ***
EDEQ_restraint   1.0369     0.1358   7.638  4.8e-14 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

*Residual standard error: 6.081 on 1092 degrees of freedom
Multiple R-squared:  0.05072,	Adjusted R-squared:  0.04985 
F-statistic: 58.34 on 1 and 1092 DF,  p-value: 4.799e-14
...
```

The residual standard error (RSE) is the standard deviation of the residuals. This summarizes the variability of observed values around the model-predicted values, **in the original units of the DV.**

`$$RSE = 6.081$$`

This means observed values vary around our model-predicted BMI with a standard deviation of 6.081. In BMI, 6 units is quite large!

---
## Evaluating a regression - R-Squared

Here is our summary of model performance.


```
...
Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     23.9383     0.2631  90.973  &lt; 2e-16 ***
EDEQ_restraint   1.0369     0.1358   7.638  4.8e-14 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.081 on 1092 degrees of freedom
*Multiple R-squared:  0.05072,	Adjusted R-squared:  0.04985 
F-statistic: 58.34 on 1 and 1092 DF,  p-value: 4.799e-14
...
```

The R-squared value is .05.

This means that our model accounts for 5% of the variance in BMI. Since our model has only one predictor, we can alternatively say Dietary Restraint accounts for 5% of the variance in BMI.

---
class: middle, inverse

# Regression inference

---
## What are we testing in a regression?

There are multiple inferential tests in a regression model:
* Test of the model (omnibus test) 
* Tests of the coefficients


```
...
Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
*(Intercept)     23.9383     0.2631  90.973  &lt; 2e-16 ***
*EDEQ_restraint   1.0369     0.1358   7.638  4.8e-14 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.081 on 1092 degrees of freedom
Multiple R-squared:  0.05072,	Adjusted R-squared:  0.04985 
*F-statistic: 58.34 on 1 and 1092 DF,  p-value: 4.799e-14
...
```


---
### F-Distributions and Omnibus Tests

The omnibus test uses the F-distribution to test the ratio of two variances (i.e., explained vs unexplained variance).

Null hypothesis: The model does not account for variance in Y.

If we reject the null, then the model accounts for more variance than we would expect by chance. 
---
### F-Distributions and Omnibus Tests

Just like tests with other probability distribution, we are testing the probability of obtaining a value (or more extreme value) of F under the F-Distribution.

`$$\Large F = \frac{MS_{Model}}{MS_{residual}}$$`

Mean Squares (MS) are the Sums of Squares divided by their respective degrees of freedom.

---
# Interpreting the Omnibus Test


```
...
Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     23.9383     0.2631  90.973  &lt; 2e-16 ***
EDEQ_restraint   1.0369     0.1358   7.638  4.8e-14 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.081 on 1092 degrees of freedom
Multiple R-squared:  0.05072,	Adjusted R-squared:  0.04985 
*F-statistic: 58.34 on 1 and 1092 DF,  p-value: 4.799e-14
...
```

Our omnibus test was significant (p &lt; .001), therefore we can reject the null hypothesis that the model accounts for zero variance in BMI.

---
### Regression Coefficients

Significance tests of regression coefficients test the null hypothesis that `\(\beta = 0\)`. (there is no linear relationship between the predictor and the outcome).

`$$\Large H_{0}: \beta_{1}= 0$$`
`$$\Large H_{1}: \beta_{1} \neq 0$$`
Here, the p-value refers to the probability of obtaining a slope equal to or more extreme than `\(b\)` assuming `\(H_0\)` is true. 

---
### Regression Coefficients

Our R output shows us the significance of the intercept. Typically, we are not interested in whether the intercept significantly differs from 0.


```
...
Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
*(Intercept)     23.9383     0.2631  90.973  &lt; 2e-16 ***
EDEQ_restraint   1.0369     0.1358   7.638  4.8e-14 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.081 on 1092 degrees of freedom
Multiple R-squared:  0.05072,	Adjusted R-squared:  0.04985 
F-statistic: 58.34 on 1 and 1092 DF,  p-value: 4.799e-14
...
```

---
### Regression Coefficients


```
...
Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     23.9383     0.2631  90.973  &lt; 2e-16 ***
*EDEQ_restraint   1.0369     0.1358   7.638  4.8e-14 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.081 on 1092 degrees of freedom
Multiple R-squared:  0.05072,	Adjusted R-squared:  0.04985 
F-statistic: 58.34 on 1 and 1092 DF,  p-value: 4.799e-14
...
```

`$$t = \frac{\hat{\beta_1} - 0}{SE(\hat{\beta_1})} = \frac{1.04}{.14} = 7.43$$`
`$$Pr(t &lt; -7.43 \text{ or } t &gt; 7.43)|H_0 = 0.000000000000048 \text{ or p &lt;.001}$$`
---
### Regression Coefficients


```
...
Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     23.9383     0.2631  90.973  &lt; 2e-16 ***
*EDEQ_restraint   1.0369     0.1358   7.638  4.8e-14 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.081 on 1092 degrees of freedom
Multiple R-squared:  0.05072,	Adjusted R-squared:  0.04985 
F-statistic: 58.34 on 1 and 1092 DF,  p-value: 4.799e-14
...
```

Under the null hypothesis, it is extremely unlikely to obtain a Dietary Restraint slope of 1.04 (p &lt; .001). Therefore, we can reject the null and conclude that there is a significant relationship between Dietary Restraint rating and BMI.
---
### Inference in Regression

In a simple regression model, we are testing for a linear relationship between a predictor and the outcome.

&lt;img src="EDUC643_1_intro_files/figure-html/unnamed-chunk-26-1.png" style="display: block; margin: auto;" /&gt;

It is important to inspect the data to make sure we are modelling correctly. Otherwise, we'd miss this non-linear relationship!
---
## Summarizing Regression Results


```
...
Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     23.9383     0.2631  90.973  &lt; 2e-16 ***
*EDEQ_restraint   1.0369     0.1358   7.638  4.8e-14 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.081 on 1092 degrees of freedom
Multiple R-squared:  0.05072,	Adjusted R-squared:  0.04985 
F-statistic: 58.34 on 1 and 1092 DF,  p-value: 4.799e-14
...
```

A simple regression model was used to describe the relationship between BMI and Dietary Restraint in young adult males. Dietary Restraint was found to be a significant predictor (p &lt; .001) and accounted for approximately 5.07% of the variance in BMI. For each one unit increase in Dietary Restraint, there is an average increase of 1.04 BMI units.

---
### Regressions as a prediction

Regression equations can be used to evaluate the relationship between variables, and to predict expected values based on particular values of our IVs.

We can ask: What is the expected BMI value for a young male with a Dietary Restraint rating of **4**?

--
`$$\Large BMI = 23.94 + 1.04*(\boldsymbol{4}) = 28.1$$`
The expected BMI for a Dietary Restraint of 4 is 28.1

--

Technically, there is no limit to what we can input!

---
### Predicting beyond your data

Regression equations can be used to evaluate the relationship between variables, and to predict expected values based on particular values of our IVs.

We can ask: What is the expected BMI value for a young male with a Dietary Restraint rating of **400**?

--

Using our measure, this is not a possible value of Dietary Restraint but we can still estimate BMI using our regression equation.

--
`$$\Large BMI = 23.94 + 1.04*(\boldsymbol{400}) = 439.94$$`
An adult would have to weight 2,894 pounds at 5' 8" to get a BMI of 439.94.

--

**Only predict within the bounds of your data.**

---
class: middle, inverse

# Correlation and causality

---
## Oregon Divorces and Beef Consumption

Does U.S. beef consumption cause more "beefs" between Oregonians and their spouses?
* What is the relationship between Oregon's per capita divorce rate and U.S. per capita beef consumption? 

--
&lt;img src="EDUC643_1_intro_files/figure-html/unnamed-chunk-28-1.png" style="display: block; margin: auto;" /&gt;

There certainly seems to be a relationship! Let's run a simple regression model.
---
## Regression Models -  Divorce and Beef

If we regress divorce rate on beef consumption...


```
...
Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)       -3.7437     2.8068  -1.334   0.2190  
*beef_consumption   0.1285     0.0451   2.849   0.0215 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.2711 on 8 degrees of freedom
Multiple R-squared:  0.5037,	Adjusted R-squared:  0.4416 
F-statistic: 8.119 on 1 and 8 DF,  p-value: 0.0215
...
```

Beef consumption accounts for 50% of the variance in Oregon's divorce rate!

---
## Regression Models -  Divorce and Beef

If we regress U.S. beef consumption on Oregon's divorce rate...


```
...
Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    45.551      5.866   7.765 5.41e-05 ***
*divorce_rate    3.920      1.376   2.849   0.0215 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.498 on 8 degrees of freedom
Multiple R-squared:  0.5037,	Adjusted R-squared:  0.4416 
F-statistic: 8.119 on 1 and 8 DF,  p-value: 0.0215
...
```

Oregon's divorce rate accounts for 50% of the variance in U.S. beef consumption?
---
## Regression Models -  Divorce and Beef

What can we conclude? - Do increases in Oregon's divorce rate **cause** increases in U.S. beef consumption, or do increases in beef consumption **cause** increases in the divorce rate?

--

**Probably neither...**

--

We tested for a linear relationship. There is a significant linear relationship or correlation between divorce rate and beef consumption, but we did not conduct an experimental study.

**We cannot establish causality, nor the direction of the relationship with a statistical test.**

--

Study design is critical.

---
## Turn and Talk

* Discuss science communication and how you would distinguish correlations from causal relationships to the average person.

* In what contexts might it be difficult to conduct an experimental study to establish causality?

---

End of Slides
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
